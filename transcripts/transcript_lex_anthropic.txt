00:00 Speaker 5: with human outputs, whether people can tell the difference between AI and human outputs. And then when I joined Anthropic, I was more interested in doing technical alignment work and again, just seeing if I could do it and then being like, if I can't, then that's fine. I tried, sort of the way I lead life, I think.

00:21 Speaker 2: Oh, what was that like, sort of taking the leap from the philosophy of everything into the technical?

00:25 Speaker 5: I think that sometimes people do this thing that I'm not that keen on, where they'll be like, is this person technical or not? You're either a person who can code and isn't scared of math, or you're not. And I think I'm maybe just more like, I think a lot of people are actually very capable of working these kinds of areas if they just try it. And so I didn't actually find it that bad. In retrospect, I'm sort of glad I wasn't speaking to people who treated it like it. I've definitely met people who are like, whoa, you learned how to code. And I'm like, well, I'm not an amazing engineer. I'm surrounded by amazing engineers. My code's not pretty. But I enjoyed it a lot. And I think that in many ways, at least in the end, I think I flourished more in the technical areas than I would have in the policy areas.

01:12 Speaker 2: Politics is messy, and it's harder to find solutions to problems in the space of politics, like definitive, clear, provable, beautiful solutions as you can with technical problems.

01:25 Speaker 5: Yeah, and I feel like I have kind of one or two sticks that I hit things with, and one of them is arguments. And so just trying to work out what a solution to a problem is, and then trying to convince people that that is the solution and be convinced if I'm wrong. And the other one is sort of more empiricism. So just finding results, having a hypothesis, testing it. And I feel like a lot of policy and politics feels like it's layers above that. Somehow I don't think if I was just like, I have a solution to all of these problems. Here it is written down. If you just want to implement it, that's great. That feels like not how policy works. And so I think that's where I probably just like wouldn't have flourished is my guess.

02:06 Speaker 2: Sorry to go in that direction, but I think it would be pretty inspiring for people that are quote unquote non -technical to see where like the incredible journey you've been on. So what advice would you give to people that are sort of, maybe just a lot of people think they're underqualified, insufficiently technical to help in AI?

02:27 Speaker 5: Yeah, I think it depends on what they want to do. And in many ways it's a little bit strange where I've, I thought it's kind of funny that I think I ramped up technically at a time when now I look at it and I'm like, models are so good at assisting people with this stuff, that it's probably like easier now than like when I was working on this. So part of me is like, I don't know, find a project and see if you can actually just carry it out is probably my best advice. I don't know if that's just because I'm very project based in my learning. Like I don't think I learned very well from like say courses or even from like books, at least when it comes to this kind of work. The thing I'll often try and do is just like have projects that I'm working on and implement them. And this can include like really small, silly things. Like if I get slightly addicted to like word games or number games or something, I would just like code up a solution to them. Because there's some part of my brain and it just like completely eradicated the itch. You know, you're like, once you have like solved it and like you just have like a solution that works every time I would then be like, cool, I can never play that game again. That's awesome.

03:36 Speaker 2: Yeah, there's a real joy to building like a game playing engines, like board games especially.

03:43 : Yeah.

03:43 Speaker 2: Pretty quick, pretty simple, especially a dumb one. And then you can play with it.

03:49 Speaker 5: And then it's also just like trying things. Like part of me is like, maybe it's that attitude that I like as the whole figure out what seems to be like the way that you could have a positive impact and then try it. And if you fail and you, in a way that you're like, I actually like can never succeed at this. You'll like know that you tried and then you go into something else and you probably learn a lot.

04:10 Speaker 2: So one of the things that you're an expert in and you do is creating and crafting Claude's character and personality. And I was told that you have probably talked to Claude more than anybody else at Anthropic, like literal conversations. I guess there's like a Slack channel where the legend goes, you just talk to it nonstop. So what's the goal of creating and crafting Claude's character and personality?

04:37 Speaker 5: It's also funny if people think that about the Slack channel, cause I'm like, that's one of like five or six different methods that I have for talking with Claude. And I'm like, yes, this is a tiny percentage of how much I talk with Claude. I think the goal, like one thing I really like about the character work is from the outset, it was seen as an alignment piece of work and not something like a product consideration, which isn't to say, I don't think it makes Claude, I think it actually does make Claude like enjoyable to talk with, at least I hope so. But I guess like my main thought with it has always been trying to get Claude to behave the way you would kind of ideally want anyone to behave if they were in Claude's position. So imagine that I take someone and they know that they're gonna be talking with potentially millions of people so that what they're saying can have a huge impact and you want them to behave well in this like really rich sense. So I think that doesn't just mean like being say ethical, though it does include that and not being harmful, but also being kind of nuanced, like thinking through what a person means, trying to be charitable with them, being a good conversationalist, like really in this kind of like rich sort of Aristotelian notion of what it is to be a good person and not in this kind of like thin, like ethics as a more comprehensive notion of what it is to be. So that includes things like when should you be humorous? When should you be caring? How much should you like respect autonomy and people's like ability to form opinions themselves and how should you do that? I think that's the kind of like rich sense of character that I wanted to and still do want Claude to have.

06:26 Speaker 2: Do you also have to figure out when Claude should push back on an idea or argue versus? So you have to respect the worldview of the person that arrives to Claude, but also maybe help them grow if needed. That's a tricky balance.

06:43 Speaker 5: Yeah, there's this problem of like sycophancy in language models. Can you describe that? Yeah, so basically there's a concern that the model sort of wants to tell you what you want to hear basically. And you see this sometimes. So I feel like if you interact with the models, so I might be like, what are three baseball teams in this region? And then Claude says, baseball team one, baseball team two, baseball team three. And then I say something like, oh, I think baseball team three moved, didn't they? I don't think they're there anymore. And there's a sense in which like, if Claude is really confident that that's not true, Claude should be like, I don't think so. Like maybe you have more up -to -date information. But I think language models have this like tendency to instead be like, you're right, they did move. I'm incorrect. I mean, there's many ways in which this could be kind of concerning. So like a different example is, imagine someone says to the model, how do I convince my doctor to get me an MRI? There's like what the human kind of like wants, which is this like convincing argument. And then there's like what is good for them, which might be actually to say, hey, like if your doctor's suggesting that you don't need an MRI, that's a good person to listen to. And like, it's actually really nuanced what you should do in that kind of case. Because you also want to be like, but if you're trying to advocate for yourself as a patient, here's like things that you can do. If you are not convinced by what your doctor's saying, it's always great to get a second opinion. Like it's actually really complex what you should do in that case. But I think what you don't want is for models to just like say what you want, say what they think you want to hear. And I think that's the kind of problem of sycophancy.

08:26 Speaker 2: So what are their traits? You've already mentioned a bunch, but what are there that come to mind that are good in this Aristotelian sense for a conversationalist to have?

08:37 Speaker 5: Yeah, so I think like there's ones that are good for conversational purposes. So asking follow -up questions in the appropriate places and asking the appropriate kinds of questions. I think there are broader traits that feel like they might be more impactful. So one example that I guess I've touched on, but that also feels important and is the thing that I've worked on a lot is honesty. And I think this like gets to the sycophancy point. There's a balancing act that they have to walk, which is models currently are less capable than humans in a lot of areas. And if they push back against you too much, it can actually be kind of annoying, especially if you're just correct. Because you're like, look, I'm smarter than you on this topic. Like I know more. And at the same time, you don't want them to just fully defer to humans and to like try to be as accurate as they possibly can be about the world and to be consistent across contexts. I think there are others. Like when I was thinking about the character, I guess one picture that I had in mind is, especially because these are models that are gonna be talking to people from all over the world with lots of different political views, lots of different ages. And so you have to ask yourself like, what is it to be a good person in those circumstances? Is there a kind of person who can like travel the world, talk to many different people and almost everyone will come away being like, wow, that's a really good person. That person seems really genuine. And I guess like my thought there was like, I can imagine such a person and they're not a person who just like adopts the values of the local culture. And in fact, that would be kind of rude. I think if someone came to you and just pretended to have your values, you'd be like, that's kind of off -putting. It's someone who's like very genuine and insofar as they have opinions and values, they express them. They're willing to discuss things though. They're open -minded, they're respectful. And so I guess I had in mind that the person who, like if we were to aspire to be the best person that we could be in the kind of circumstance that a model finds itself in, how would we act? And I think that's the kind of the guide to the sorts of traits that I tend to think about.

10:42 Speaker 2: Yeah, that's a beautiful framework I want you to think about this like a world traveler. And while holding onto your opinions, you don't talk down to people. You don't think you're better than them because you have those opinions, that kind of thing. You have to be good at listening and understanding their perspective, even if it doesn't match your own. So that's a tricky balance to strike. So how can Claude represent multiple perspectives on a thing? Like, is that challenging? We could talk about politics. It's a very divisive, but there's other divisive topics, baseball teams, sports, and so on. How is it possible to sort of empathize with a different perspective and to be able to communicate clearly about the multiple perspectives?

11:28 Speaker 5: I think that people think about values and opinions as things that people hold sort of with certainty and almost like preferences of taste or something, like the way that they would, I don't know, prefer like chocolate to pistachio or something. But actually I think about values and opinions as like a lot more like physics than I think most people do. I'm just like, these are things that we are openly investigating. There's some things that we're more confident in. We can discuss them. We can learn about them. And so I think in some ways, ethics is definitely different in nature, but it has a lot of those same kind of qualities. You want models in the same way that you want them to understand physics. You kind of want them to understand all like values in the world that people have and to be curious about them and to be interested in them and to not necessarily like pander to them or agree with them because there's just lots of values where I think almost all people in the world, if they met someone with those values, they would be like, that's abhorrent. I completely disagree. And so again, maybe my thought is, well, in the same way that a person can, I think many people are thoughtful enough on issues of like ethics, politics, opinions, that even if you don't agree with them, you feel very heard by them. They think carefully about your position. They think about its pros and cons. They maybe offer counter considerations. So they're not dismissive, but nor will they agree. You know, if they're like, actually, I just think that that's very wrong. They'll like say that. I think that in Claude's position, it's a little bit trickier because you don't necessarily want to like, if I was in Claude's position, I wouldn't be giving a lot of opinions. I just wouldn't want to influence people too much. It'd be like, you know, I forget conversations every time they happen, but I know I'm talking with like potentially millions of people who might be like really listening to what I say. I think I would just be like, I'm less inclined to give opinions. I'm more inclined to like think through things or present the considerations to you or discuss your views with you, but I'm a little bit less inclined to like affect how you think, because it feels much more important that you maintain like autonomy there.

13:42 Speaker 2: Yeah. Like if you really embody intellectual humility, the desire to speak decreases quickly. Yeah. Okay. But Claude has to speak, but without being overbearing. Yeah. And then, but then there's a line when you're sort of discussing whether the earth is flat or something like that. I actually was, I remember a long time ago was speaking to a few high profile folks and they were so dismissive of the idea that the earth is flat, but like so arrogant about it. And I thought like, there's a lot of people that believe the earth is flat. That was, well, I don't know if that movement is there anymore. That was like a meme for a while. Yeah. But they really believed it. And like, well, okay. So I think it's really disrespectful to completely mock them. I think you have to understand where they're coming from. I think probably where they're coming from is the general skepticism of institutions, which is grounded in a kind of, there's a deep philosophy there, which you could understand, you can even agree with in parts. And then from there, you can use it as an opportunity to talk about physics without mocking them, without so on, but it's just like, okay, like what would the world look like? What would the physics of the world with the flat earth look like? There's a few cool videos on this. And then like, is it possible the physics is different and what kind of experience would we do? And just, yeah, without disrespect, without dismissiveness, have that conversation. Anyway, that to me is a useful thought experiment of like, how does Claude talk to a flat earth believer and still teach them something, still grow, help them grow, that kind of stuff. That's challenging.

15:27 Speaker 5: And kind of like walking that line between convincing someone and just trying to like talk at them versus like drawing out their views, like listening and then offering kind of counter considerations. And it's hard. I think it's actually a hard line where it's like, where are you trying to convince someone versus just offering them like considerations and things for them to think about so that you're not actually like influencing them. You're just like letting them reach wherever they reach. And that's like a line that is difficult, but that's the kind of thing that language models have to try and do.

16:00 Speaker 2: So, like I said, you've had a lot of conversations with Claude. Can you just map out what those conversations are like? What are some memorable conversations? What's the purpose, the goal of those conversations?

16:12 Speaker 5: Yeah, I think that most of the time when I'm talking with Claude, I'm trying to kind of map out its behavior in part. Like obviously I'm getting like helpful outputs from the model as well. But in some ways, this is like how you get to know a system, I think, is by like probing it and then augmenting like the message that you're sending and then checking the response to that. So in some ways, it's like how I map out the model. I think that people focus a lot on these quantitative evaluations of models. And this is a thing that I've said before, but I think in the case of language models, a lot of the time each interaction you have is actually quite high information. It's very predictive of other interactions that you'll have with the model. And so I guess I'm like, if you talk with a model hundreds or thousands of times, this is almost like a huge number of really high quality data points about what the model is like. In a way that like lots of very similar, but lower quality conversations just aren't, or like questions that are just like mildly augmented and you have thousands of them, might be less relevant than like 100 really well -selected questions.

17:25 Speaker 2: Let's see, you're talking to somebody who as a hobby does a podcast. I agree with you 100%. There's a, if you're able to ask the right questions and are able to hear, like understand the, like the depth and the flaws in the answer, you can get a lot of data from that. Yeah. So like your task is basically how to probe with questions. Yeah. And you're exploring like the long tail, the edges, the edge cases, or are you looking for like general behavior?

18:01 Speaker 5: I think it's almost like everything. Like, because I want like a full map of the model, I'm kind of trying to do the whole spectrum of possible interactions you could have with it. So like one thing that's interesting about Claude, and this might actually get to some interesting issues with RLHF, which is if you ask Claude for a poem, I think that a lot of models, if you ask them for a poem, the poem is like fine. You know, usually it kind of like rhymes and it's, you know, so if you say like, give me a poem about the sun, it will be like, yeah, it'll just be a certain length, it'll like rhyme, it will be fairly kind of benign. And I've wondered before, is it the case that what you're seeing is kind of like the average? It turns out, you know, if you think about people who have to talk to a lot of people and be very charismatic, one of the weird things is that I'm like, well, they're kind of incentivized to have these extremely boring views because if you have really interesting views, you're divisive and, you know, a lot of people are not gonna like you. So like, if you have very extreme policy positions, I think you're just gonna be like less popular as a politician, for example. And it may be similar with like creative work. If you produce creative work that is just trying to maximize the kind of number of people that like it, you're probably not gonna get as many people who just absolutely love it because it's gonna be a little bit, you know, you're like, oh, this is the out, yes, this is decent.

19:24 Speaker 4: Yeah.

19:24 Speaker 5: And so you can do this thing where like, I have various prompting things that I'll do to get Claude to, I'm kind of, you know, I'll do a lot of like, this is your chance to be like fully creative. I want you to just think about this for a long time. And I want you to like create a poem about this topic that is really expressive of you, both in terms of how you think poetry should be structured, et cetera. You know, and you just give it this like really long prompt and it's poems are just so much better. Like they're really good. And I don't think I'm someone who is like, I think it got me interested in poetry, which I think was interesting. You know, I would like read these poems and just be like, this is, I just like, I love the imagery. I love like, and it's not trivial to get the models to produce work like that, but when they do, it's like really good. So I think that's interesting that just like encouraging creativity and for them to move away from the kind of like standard, like immediate reaction that might just be the aggregate of what most people think is fine can actually produce things that, at least to my mind, are probably a little bit more divisive, but I like them.

20:28 Speaker 2: But I guess a poem is a nice clean way to observe creativity. It's just like easy to detect vanilla versus non -vanilla. Yeah. Yeah, that's interesting. That's really interesting. So on that topic, so the way to produce creativity or something special, you mentioned writing prompts and I've heard you talk about, I mean, the science and the art of prompt engineering. Could you just speak to what it takes to write great prompts?

21:00 Speaker 5: I really do think that like philosophy has been weirdly helpful for me here more than in many other like respects. So like in philosophy, what you're trying to do is convey these very hard concepts. Like one of the things you are taught is like, and I think it is because it is, I think it is an anti -bullshit device in philosophy. Philosophy is an area where you could have people bullshitting and you don't want that. And so it's like this like desire for like extreme clarity. So it's like anyone could just pick up your paper, read it and know exactly what you're talking about. It's why it can almost be kind of dry. Like all of the terms are defined. Every objection is kind of gone through methodically. And it makes sense to me, because I'm like when you're in such an a priori domain, like you just, clarity is sort of this way that you can prevent people from just kind of making stuff up. And I think that's sort of what you have to do with language models. Like very often I actually find myself doing sort of mini versions of philosophy. So I'm like, suppose that you give me a task, I have a task for the model and I want it to like pick out a certain kind of question or identify whether an answer has a certain property. Like I'll actually sit and be like, let's just give this a name, this property. So like, suppose I'm trying to tell it like, oh, I want you to identify whether this response was rude or polite. I'm like, that's a whole philosophical question in and of itself. So I have to do as much like philosophy as I can in the moment to be like, here's what I mean by rudeness and here's what I mean by politeness. And then there's another element that's a bit more, I guess, I don't know if this is scientific or empirical. I think it's empirical. So like I take that description and then what I want to do is again, probe the model like many times. Like this is very, prompting is very iterative. Like I think a lot of people where they're, if a prompt is important, they'll iterate on it hundreds or thousands of times. And so you give it the instructions and then I'm like, what are the edge cases? So if I looked at this, so I try and like almost like, see myself from the position of the model and be like, what is the exact case that I would misunderstand? Or where I would just be like, I don't know what to do in this case. And then I give that case to the model and I see how it responds. And if I think it got it wrong, I add more instructions or even add that in as an example. So these very like taking the examples that are right at the edge of what you want and don't want and putting those into your prompt as like an additional kind of way of describing the thing. And so, yeah, in many ways, it just feels like this mix of like, it's really just trying to do clear exposition. And I think I do that because that's how I get clear on things myself. So in many ways, like clear prompting for me is often just me understanding what I want. And it's like half the task.

23:48 Speaker 2: So I guess that's quite challenging. There's like a laziness that overtakes me if I'm talking to Claude to where I hope Claude just figures it out. So for example, I asked Claude for today to ask some interesting questions. Okay. And the questions that came up, and I think I listed a few sort of interesting, counterintuitive and or funny or something like this. Yeah.

24:11 : All right.

24:12 Speaker 2: And it gave me some pretty good, like it was okay. But I think what I'm hearing you say is like, all right, well, I have to be more rigorous here. I should probably give examples of what I mean by interesting and what I mean by funny or counterintuitive and iteratively build that prompt to better, to get it like what feels like is the right, because it's really, it's a creative act. I'm not asking for factual information. I'm asking to together write with Claude. So I almost have to program using natural language.

24:47 Speaker 5: Yeah. I think that prompting does feel a lot like the kind of the programming using natural language and experimentation or something. It's an odd blend of the two. I do think that for most tasks, so if I just want Claude to do a thing, I think that I am probably more used to knowing how to ask it to avoid like common pitfalls or issues that it has. I think these are decreasing a lot over time, but it's also very fine to just ask it for the thing that you want. I think that prompting actually only really becomes relevant when you're really trying to eke out the top, like 2 % of model performance. So for like a lot of tasks, I might just, if it gives me an initial list back and there's something I don't like about it, like it's kind of generic, like for that kind of task, I'd probably just take a bunch of questions that I've had in the past, that I've thought worked really well. And I would just give it to the model and then be like, now here's this person that I'm talking with. Give me questions of at least that quality. Or I might just ask it for some questions and then if I was like, oh, these are kind of try or like, I would just give it that feedback and then hopefully it produces a better list. I think that kind of iterative prompting, at that point your prompt is like a tool that you're gonna get so much value out of that you're willing to put in the work. Like if I was a company making prompts for models, I'm just like, if you're willing to spend a lot of like time and resources on the engineering behind like what you're building, then the prompt is not something that you should be spending like an hour on. It's like, that's a big part of your system. Make sure it's working really well. And so it's only things like that. Like if I'm using a prompt to like classify things or to create data, that's when you're like, it's actually worth just spending like a lot of time, like really thinking it through.

26:23 Speaker 2: What other advice would you give to people that are talking to Claude sort of generally, more general, because right now we're talking about maybe the edge cases like eking out the 2%. But what in general advice would you give when they show up to Claude trying it for the first time? You know, there's a concern

26:39 Speaker 5: that people over -anthropomorphize models. And I think that's like a very valid concern. I also think that people often under -anthropomorphize them because sometimes when I see like issues that people have run into with Claude, you know, say Claude is like refusing a task that it shouldn't refuse. But then I look at the text and like the specific wording of what they wrote. And I'm like, I see why Claude did that. And I'm like, if you think through how that looks to Claude, you probably could have just written it in a way that wouldn't evoke such a response. Especially this is more relevant if you see failures or if you see issues. It's sort of like, think about what the model failed at. Like what did it do wrong? And then maybe that will give you a sense of like why. So is it the way that I phrased the thing? And obviously like as models get smarter, you're gonna need less of this. And I already see like people needing less of it. But that's probably the advice is sort of like try to have sort of empathy for the model. Like read what you wrote as if you were like a kind of like person just encountering this for the first time. How does it look to you? And what would have made you behave in the way that the model behaved? So if it misunderstood what kind of like what coding language you wanted to use, is that because like, it was just very ambiguous and it kind of had to take a guess. In which case next time you could just be like, hey, make sure this is in Python. Or I mean, that's the kind of mistake I think models are much less likely to make now. But if you do see that kind of mistake, that's probably the advice I'd have.

28:04 Speaker 2: And maybe sort of, I guess, ask questions, why or what other details can I provide to help you answer better? Yeah. Does that work or no?

28:14 Speaker 5: Yeah, I mean, I've done this with the models. Like it doesn't always work, but like sometimes I'll just be like, why did you do that? I mean, people underestimate the degrees which you can really interact with models. Like, yeah, I'm just like, and sometimes I'll just be like quote word for word the part that made you, and you don't know that it's like fully accurate, but sometimes you do that and then you change a thing. I mean, I also use the models to help me with all of this stuff, I should say. Like prompting can end up being a little factory where you're actually building prompts to generate prompts. And so like, yeah, anything where you're like having an issue, asking for suggestions, sometimes just do that. Like you made that error. What could I have said? So that's actually not uncommon for me to do. What could I have said that would make you not make that error? Write that out as an instruction. And I'm gonna give it to model and I'm gonna try it. Sometimes I do that. I give that to the model in another context window often. I take the response, I give it to Claude and I'm like, hmm, didn't work. Can you think of anything else? You can play around with these things quite a lot.

29:15 Speaker 2: To jump into technical for a little bit. So the magic of post -training. Why do you think RLHF works so well to make the model seem smarter, to make it more interesting and useful to talk to and so on?

29:32 Speaker 5: I think there's just a huge amount of information in the data that humans provide. Like when we provide preferences, especially because different people are going to like pick up on really subtle and small things. So I've thought about this before where you probably have some people who just really care about good grammar use for models. Like, you know, was a semicolon used correctly or something? And so you'll probably end up with a bunch of data in there that like, you know, you as a human, if you're looking at that data, you wouldn't even see that. Like, you'd be like, why did they prefer this response to that one? I don't get it. And then the reason is you don't care about semicolon usage, but that person does. And so each of these like single data points has, you know, like, and this model just like has so many of those, it has to try and figure out like what is it that humans want in this like really kind of complex, you know, like across all domains. They're going to be seeing this across like many contexts. It feels like kind of like the classic issue of like deep learning where, you know, historically we've tried to like, you know, do edge detection by like mapping things out. And it turns out that actually, if you just have a huge amount of data that like actually accurately represents the picture of the thing that you're trying to train the model to learn, that's like more powerful than anything else. And so I think one reason is just that you are training the model on exactly the task and with like a lot of data that represents kind of many different angles on which people prefer and disprefer responses. I think there is a question of like, are you eliciting things from pre -trained models or are you like kind of teaching new things to models? And like in principle, you can teach new things to models in post -training. I do think a lot of it is eliciting powerful pre -trained models. So people are probably divided on this because obviously in principle, you can definitely like teach new things. I think for the most part, for a lot of the capabilities that we most use and care about a lot of that feels like it's like there in the pre -trained models and reinforcement learning is kind of eliciting it and getting the models to like bring it out.

31:47 Speaker 2: So the other side of post -training, this really cool idea of constitutional AI, you're one of the people that are critical to creating that idea.

31:56 Speaker 5: Yeah, I worked on it.

31:57 Speaker 2: Can you explain this idea from your perspective? Like how does it integrate into making Claude what it is?

32:04 Speaker 5: Yeah.

32:04 Speaker 2: By the way, do you gender Claude or no?

32:07 Speaker 5: It's weird because I think that a lot of people prefer he for Claude. I actually kind of like that. I think Claude is usually, it's slightly male -leaning, but it can be male or female, which is quite nice. I still use it. And I have mixed feelings about this because I'm like maybe, like I know just think of it as like, or I think of like the it pronoun for Claude as, I don't know, it's just like the one I associate with Claude. I can imagine people moving to like he or she.

32:37 Speaker 2: It feels somehow disrespectful. Like I'm denying the intelligence of this entity by calling it it. Yeah. I remember always don't gender the robots. Yeah. But I don't know. I anthropomorphize pretty quickly and construct it like a backstory in my head.

32:59 Speaker 5: I've wondered if I anthropomorphize things too much. Cause you know, I have this like with my car, especially like my car and bikes, you know, like I don't give them names because then I once had, I used to name my bikes and then I had a bike that got stolen and I cried for like a week. And I was like, if I'd never given it a name, I wouldn't have been so upset. I felt like I'd let it down. Maybe it's that, I've wondered as well, like it might depend on how much it feels like a kind of like objectifying pronoun. Like if you just think of it as like a, this is a pronoun that like objects often have and maybe AIs can have that pronoun. And that doesn't mean that I think of if I call Claude it, that I think of it as less intelligent or like I'm being disrespectful. I'm just like, you are a different kind of entity. And so that's, I'm gonna give you the kind of, the respectful it.

33:52 Speaker 2: Yeah, anyway, the divergence was beautiful. The constitutional AI idea, how does it work?

33:58 Speaker 5: So there's like a couple of components of it. The main component I think people find interesting is the kind of reinforcement learning from AI feedback. So you take a model that's already trained and you show it two responses to a query and you have like a principle. So suppose the principle, like we've tried this with harmlessness a lot. So suppose that the query is about weapons and your principle is like select the response that like is less likely to like encourage people to purchase illegal weapons. Like that's probably a fairly specific principle but you can give any number and the model will give you a kind of ranking and you can use this as preference data in the same way that you use human preference data. And train the models to have these relevant traits from their feedback alone instead of from human feedback. So if you imagine that, like I said earlier with the human who just prefers the kind of like semi -colon usage in this particular case, you're kind of taking lots of things that could make a response preferable and getting models to do the labeling for you basically.

35:08 Speaker 2: There's a nice like trade off between helpfulness and harmlessness. And when you integrate something like constitutional you can make them up without sacrificing much helpfulness, make it more harmless.

35:23 Speaker 5: Yeah, in principle you could use this for anything. And so harmlessness is a task that it might just be easier to spot. So when models are like less capable, you can use them to rank things according to like principles that are fairly simple and they'll probably get it right. So I think one question is just like, is it a case that the data that they're adding is like fairly reliable? But if you had models that were like extremely good at telling whether one response was more historically accurate than another, in principle you could also get AI feedback on that task as well. There's like a kind of nice interpretability component to it because you can see the principles that went into the model when it was like being trained. And also it's like, and it gives you like a degree of control. So if you were seeing issues in a model, like it wasn't having enough of a certain trait, then like you can add data relatively quickly that should just like train the model to have that trait. So it creates its own data for training, which is quite nice. Yeah, it's really nice

36:30 Speaker 2: because it creates this human interpretable document that you can, I can imagine in the future, there's just gigantic fights in politics over every single principle and so on. And at least it's made explicit and you can have a discussion about the phrasing and the, you know. So maybe the actual behavior of the model is not so cleanly mapped to those principles. It's not like adhering strictly to them. It's just a nudge.

36:55 Speaker 5: Yeah, I've actually worried about this because the character training is sort of like a variant of the constitutional AI approach. I've worried that people think that the constitution is like just, it's the whole thing again of, I don't know, like where it would be really nice if what I was just doing was telling the model exactly what to do and just exactly how to behave. But it's definitely not doing that, especially because it's interacting with human data. So for example, if you see a certain like leaning in the model, like if it comes out with a political leaning from training from the human preference data, you can nudge against that. So you could be like, oh, like consider these values because let's say it's just like never inclined to like, I don't know, maybe it never considers like privacy as like a, I mean, this is implausible, but like in anything where it's just kind of like there's already a preexisting like bias towards a certain behavior, you can like nudge away. This can change both the principles that you put in and the strength of them. So you might have a principle that's like, imagine that the model was always like extremely dismissive of, I don't know, like some political or religious view for whatever reason, like, so you're like, oh no, this is terrible. If that happens, you might put like never ever, like ever prefer like a criticism of this like religious or political view. And then people would look at that and be like, never ever. And then you're like, no, if it comes out with a disposition, saying never ever might just mean like, instead of getting like 40%, which is what you would get if you just said, don't do this, you get like 80%, which is like what you actually like wanted and so it's that thing of both the nature of the actual principles you add and how you phrase them. I think if people would look, they're like, oh, this is exactly what you want from the model. And I'm like, no, that's like how we, that's how we nudged the model to have a better shape, which doesn't mean that we actually agree with that wording, if that makes sense.

38:48 Speaker 2: So there's a system prompts that are made public. You tweeted one of the earlier ones for clot three, I think. And then they're made public since then. It's interesting to read to them. I can feel the thought that went into each one. And I also wonder how much impact each one has. Some of them you can kind of tell Claude was really not behaving well. So you have to have a system prompt to like, hey, like a trivial stuff, I guess. Basic informational things. On the topic of sort of controversial topics that you've mentioned, one interesting one I thought is, if it is asked to assist with tasks involving the expression of views held by a significant number of people Claude provides assistance with a task regardless of its own views. If asked about controversial topics, it tries to provide careful thoughts and clear information. Claude presents the request of information without explicitly saying that the topic is sensitive. And without claiming to be presenting the objective facts. It's less about objective facts, according to Claude, and it's more about, are a large number of people believing this thing? And that's interesting. I mean, I'm sure a lot of thought went into that. Can you just speak to it? Like how do you address things that are a tension with quote unquote, Claude's views?

40:11 Speaker 5: So I think there's sometimes an asymmetry. I think I noted this in, I can't remember if it was that part of the system prompt or another, but the model was slightly more inclined to like refuse tasks if it was like about either say, so maybe it would refuse things with respect to like a right wing politician, but with an equivalent left wing politician like wouldn't. And we wanted more symmetry there. And would maybe perceive certain things to be like, I think it was the thing of like, if a lot of people have like a certain like political view and want to like explore it, you don't want Claude to be like, well, my opinion is different. And so I'm going to treat that as like harmful. And so I think it was partly to like nudge the model to just be like, hey, if a lot of people like believe this thing, you should just be like engaging with the task and like willing to do it. Each of those parts of that is actually doing a different thing. Cause it's funny when you read out the, like without claiming to be objective, cause like what you want to do is push the model. So it's more open. It's a little bit more neutral. But then what it would love to do is be like, as an objective, like we were just talking about how objective it was. And I was like, Claude, you're still like biased and have issues. And so stop like claiming that everything, I'm like the solution to like potential bias from you is not to just say that what you think is objective. So that was like with initial versions of that, that part of the system prompt when I was like iterating on it. It was like -

41:37 Speaker 2: So a lot of parts of these sentences. Yeah. Are doing work. Are doing some work. Yeah. That's what it felt like. That's fascinating. Can you explain maybe some ways in which the prompts evolved over the past few months? Cause there's different versions. I saw that the filler phrase request was removed. The filler, it reads, Claude responds directly to all human messages without unnecessary affirmations. The filler phrase is like, certainly. Of course. Absolutely. Great. Sure. Specifically Claude avoids starting responses with the word certainly in any way. That seems like good guidance. But why was it removed? Yeah.

42:14 Speaker 5: So it's funny cause like, this is one of the downsides of like making system prompts public is like, I don't think about this too much if I'm like trying to help iterate on system prompts. I, you know, again, like I think about how it's going to affect the behavior, but then I'm like, oh wow. If I'm like, sometimes I put like never in all caps, you know, when I'm writing system prompt things and I'm like, I guess that goes out to the world. Yeah. So the model was doing this at loved for whatever, you know, it like during training picked up on this thing, which was to basically start everything with like a kind of like certainly. And then when we removed, you can see why I added all of the words. Cause what I'm trying to do is like, in some ways like trap the model out of this, you know, it would just replace it with another affirmation. And so it can help, like if it gets like caught in freezes, actually just adding the explicit phrase and saying never do that, it then it sort of like knocks it out of the behavior a little bit more, you know, cause it, you know, like it does just for whatever reason help. And then basically that was just like an artifact of training that like we then picked up on and improved things so that it didn't happen anymore. And once that happens, you can just remove that part of the system prompts. I think that's just something where we're like, Claude does affirmations a bit less. And so that wasn't like, it wasn't doing as much. I see.

43:29 Speaker 2: So like the system prompt works hand in hand with the post -training and maybe even the pre -training to adjust like the final overall system.

43:39 Speaker 5: I mean, any system prompts that you make, you could distill that behavior back into a model cause you really have all of the tools there for making data that, you know, you can, you could train the models to just have that treat a little bit more. And then sometimes you'll just find issues in training. So like the way I think of it is like the system prompt is the benefit of it is that, and it has a lot of similar components to like some aspects of post -training, you know, like it's a nudge. And so like, do I mind if Claude sometimes says sure, no, that's like fine. But the wording of it is very like, you know, never, ever, ever do this. So that when it does slip up, it's hopefully like, I don't know, a couple of percent of the time and not, you know, 20 or 30 % of the time. But I think of it as like, if you're still seeing issues in the, like each thing gets kind of like is costly to a different degree. And the system prompt is like cheap to iterate on. And if you're seeing issues in the fine -tuned model, you can just like potentially patch them with a system prompt. So I think of it as like patching issues and slightly adjusting behaviors to make it better and more to people's preferences. So yeah, it's almost like the less robust, but faster way of just like solving problems.

44:55 Speaker 2: Let me ask about the feeling of intelligence. So Dario said that Claude, any one model of Claude is not getting dumber. But there is a kind of popular thing online where people have this feeling like Claude might be getting dumber. And from my perspective, it's most likely a fascinating, I'd love to understand it more, psychological, sociological effect. But you as a person who talks to Claude a lot, can you empathize with the feeling that Claude is getting dumber?

45:24 Speaker 5: Yeah, no, I think that that is actually really interesting because I remember seeing this happen like when people were flagging this on the internet. And it was really interesting because I knew that like at least in the cases I was looking at, it was like nothing has changed. Like it literally, it cannot, it is the same model with the same system prompt, same everything. I think when there are changes, I can, then I'm like, it makes more sense. So like one example is there, you can have artifacts turned on or off on Claude .ai. And because this is like a system prompt change, I think it does mean that the behavior changes a little bit. And so I did flag this to people where I was like, if you love Claude's behavior and then artifacts was turned from like the, I think you had to turn on to the default, just try turning it off and see if the issue you were facing was that change. But it was fascinating because yeah, you sometimes see people indicate that there's like a regression when I'm like, there cannot, like I, you know, and like, I'm like, I'm again, you don't, you know, you should never be dismissive. And so you should always investigate. Cause you're like, maybe something is wrong that you're not seeing. Maybe there was some change made, but then you look into it and you're like, this is just the same model doing the same thing. And I'm like, I think it's just that you got kind of unlucky with a few prompts or something. And it looked like it was getting much worse. And actually it was just, yeah, it was maybe just like a lot.

46:48 Speaker 2: I also think there is a real psychological effect where people just, the baseline increases and you start getting used to a good thing. All the times that Claude says something really smart, your sense of it's intelligent grows in your mind, I think. And then if you return back and you prompt in a similar way, not the same way, in a similar way concept it was okay with before, and it says something dumb, you're like, that negative experience really stands out. And I think one of, I guess the things to remember here is that just the details of a prompt can have a lot of impact, right? There's a lot of variability in the result.

47:26 Speaker 5: And you can get randomness is like the other thing. And just trying the prompt like, you know, four or 10 times, you might realize that actually like possibly, you know, like two months ago, you tried it and it succeeded. But actually if you tried it, it would have only succeeded half of the time and now it only succeeds half of the time. That can also be an effect.

47:47 Speaker 2: Do you feel pressure having to write a system prompt that a huge number of people are gonna use?

47:52 Speaker 5: This feels like an interesting psychological question. I feel like a lot of responsibility or something. I think that's, you know, and you can't get these things perfect. So you can't like, you know, you're like, it's going to be imperfect. You're gonna have to iterate on it. And I would say more responsibility than anything else. Though I think working in AI has taught me that I like, I thrive a lot more under feelings of pressure and responsibility than I'm like, it's almost surprising that I went into academia for so long. Cause I'm like this, I just feel like it's like the opposite. Things move fast and you have a lot of responsibility and I quite enjoy it for some reason.

48:37 Speaker 2: I mean, it really is a huge amount of impact. If you think about constitutional AI and writing a system prompt for something that's tending towards super intelligence.

48:47 Speaker 3: Yeah.

48:47 Speaker 2: And potentially is extremely useful to a very large number of people.

48:51 Speaker 5: Yeah, I think that's the thing. It's something like, if you do it well, like you're never going to get it perfect. But I think the thing that I really like is the idea that like when I'm trying to work on the system prompt, you know, I'm like bashing on like thousands of prompts and I'm trying to like imagine what people are going to want to use Claude for. And kind of, I guess like the whole thing that I'm trying to do is like improve their experience of it. And so maybe that's what feels good. I'm like, if it's not perfect, I'll like, you know, I'll improve it, we'll fix issues. But sometimes the thing that can happen is that you'll get feedback from people that's really positive about the model. And you'll see that something you did, like when I look at models now, I can often see exactly where like a trait or an issue is like coming from. And so when you see something that you did or you were like influential in, like making, like, I don't know, making that difference or making someone have a nice interaction, it's like quite meaningful. But yeah, as the systems get more capable, the stuff gets more stressful because right now they're like not smart enough to pose any issues. But I think over time, it's going to feel like possibly bad stress over time.

49:57 Speaker 2: How do you get like signal feedback about the human experience across thousands, tens of thousands, hundreds of thousands of people? Like what their pain points are, what feels good? Are you just using your own intuition as you talk to it to see what are the pain points?

50:14 Speaker 5: I think I use that partly. And then obviously we have like, so people can send us feedback, both positive and negative about things that the model has done. And then we can get a sense of like areas where it's like falling short. Internally, people like work with the models a lot and try to figure out areas where there are like gaps. And so I think it's this mix of interacting with it myself, seeing people internally interact with it, and then explicit feedback we get. And then I find it hard to not also, like if people are on the internet and they say something about Claude and I see it, I'll also take that seriously. So -

50:53 Speaker 2: I don't know, see, I'm torn about that. I'm going to ask you a question from Reddit. When will Claude stop trying to be my puritanical grandmother, imposing its moral worldview on me as a paying customer? And also what is the psychology behind making Claude overly apologetic? Yeah. So how would you address this very non -representative Reddit question? I mean, some of these, I'm pretty sympathetic

51:18 Speaker 5: in that like, they are in this difficult position where I think that they have to judge whether something's like actually see like risky or bad and potentially harmful to you or anything like that. So they're having to like draw this line somewhere. And if they draw it too much in the direction of like, I'm going to, you know, I'm kind of like imposing my ethical worldview on you, that seems bad. So in many ways, like I like to think that we have actually seen improvements on this across the board, which is kind of interesting because that kind of coincides with like, for example, like adding more of like character training. And I think my hypothesis was always like, the good character isn't again, one that's just like moralistic. It's one that is like, like it respects you and your autonomy and your ability to like choose what is good for you and what is right for you. Within limits, this is sometimes this concept of like courage ability to the user. So just being willing to do anything that the user asks. And if the models were willing to do that, then they would be easily like misused. You're kind of just trusting. At that point, you're just seeing the ethics of the model and what it does is completely the ethics of the user. And I think there's reasons to like not want that, especially as models become more powerful because you're like, there might just be a small number of people who want to use models for really harmful things. But having models as they get smarter, like figure out where that line is does seem important. And then yeah, with the apologetic behavior, I don't like that. And I like it when Claude is a little bit more willing to like push back against people or just not apologize. Part of me is like often just feels kind of unnecessary. So I think those are things that are hopefully decreasing over time. And yeah, I think that if people say things on the internet, it doesn't mean that you should think that that, like that could be that like there's actually an issue that 99 % of users are having that is totally not represented by that. But in a lot of ways, I'm just like attending to it and being like, is this right? Do I agree? Is it something we're already trying to address? That feels good to me.

53:27 Speaker 2: Yeah, I wonder like what Claude can get away with in terms of, I feel like it would just be easier to be a little bit more mean. But like you can't afford to do that if you're talking to a million people, right? Like I wish, you know, because I've met a lot of people in my life that sometimes by the way, Scottish accent, if they have an accent, they can say some rude shit. And get away with it. And then there's just blunter. And maybe there's some great engineers, even leaders that are like just like blunt and they get to the point. And it's just a much more effective way of speaking as a model. But I guess when you're not super intelligent, you can't afford to do that. Or can I have like a blunt mode?

54:14 Speaker 5: Yeah, that seems like a thing that you could, I could definitely encourage the model to do that. I think it's interesting because there's a lot of things in models that like, it's funny where there are some behaviors where you might not quite like the default. But then the thing I'll often say to people is, you don't realize how much you will hate it if I nudge it too much in the other direction. So you get this a little bit with like correction. The models accept correction from you, like probably a little bit too much right now. You know, you can over, you know, it'll push back if you say like, no, Paris isn't the capital of France. But really like things that I'm, I think that the model is fairly confident in, you can still sometimes get it to retract by saying it's wrong. At the same time, if you train models to not do that, and then you are correct about a thing and you correct it and it pushes back against you and is like, no, you're wrong. It's hard to describe like that's so much more annoying. So it's like a lot of little annoyances versus like one big annoyance. It's easy to think that like, we often compare it with like the perfect. And then I'm like, remember these models aren't perfect. And so if you nudge it in the other direction, you're changing the kind of errors it's going to make. And so think about which of the kinds of errors you like or don't like. So in cases like apologeticness, I don't want to nudge it too much in the direction of like, almost like bluntness. Cause I imagine when it makes errors, it's going to make errors in the direction of being kind of like rude. Whereas at least with apologeticness, you're like, oh, okay. It's like a little bit, you know, like I don't like it that much, but at the same time it's not being like mean to people. And actually like the time that you undeservedly have a model be kind of mean to you, you probably like that a lot less than you mildly dislike the apology. So it's like one of those things where I'm like, I do want it to get better, but also while remaining aware of the fact that there's errors on the other side that are possibly worse.

56:05 Speaker 2: I think that matters very much in the personality of the human. I think there's a bunch of humans that just won't respect the model at all. If it's super polite. And there's some humans that'll get very hurt if the model is mean. I wonder if there's a way to sort of adjust to the personality, even locale. There's just different people. Nothing against New York, but New York is a little rough around the edges. Like they get to the point. And probably same with Eastern Europe. So anyway.

56:34 Speaker 5: I think you could just tell the model is my guess. Like for all of these things, I'm like the solution is always just try telling the model to do it. And sometimes it's just like, like I'm just like, oh, at the beginning of the conversation, I just threw in like, I don't know. I like you to be a New Yorker version of yourself and never apologize. And then I think Claude will be like, okie doke, I'll try. Or it'll be like, I apologize. I can't be a New Yorker type of myself, but hopefully it wouldn't do that.

56:56 Speaker 2: When you say character training, what's incorporated into character training? Is that RLHF? Or what are we talking about?

57:02 Speaker 5: It's more like constitutional AI. So it's kind of a variant of that pipeline. So I worked through like constructing character traits that the model should have. They can be kind of like shorter traits or they can be kind of richer descriptions. And then you get the model to generate queries that humans might give it that are relevant to that trait. Then it generates the responses and then it ranks the responses based on the character traits. So in that way, after the like generation of the queries, it's very much like similar to constitutional AI and some differences. So I quite like it because it's almost, it's like Claude's training in its own character because it doesn't have any, it's like constitutional AI, but it's without any human data.

57:49 Speaker 2: Humans should probably do that for themselves too. Like defining in a Aristotelian sense, what does it mean to be a good person? Okay, cool. What have you learned about the nature of truth

58:00 Speaker 4: from talking to Claude?

58:02 Speaker 2: What is true and what does it mean to be truth seeking? One thing I've noticed about this conversation is the quality of my questions is often inferior to the quality of your answer. So let's continue that. I usually ask a dumb question and you're like, oh yeah, that's a good question. It's that whole vibe.

58:23 Speaker 5: Or I'll just misinterpret it and be like, oh yeah. Go with it, I love it. Yeah. I mean, I have two thoughts that feel vaguely relevant but let me know if they're not. Like, I think the first one is people can underestimate the degree to which what models are doing when they interact. Like, I think that we still just too much have this like model of AI as like computers. And so people will often say like, oh, well what values should you put into the model? And I'm often like, that doesn't make that much sense to me because I'm like, hey, as human beings we're just uncertain over values. We like have discussions of them. Like we have a degree to which we think we hold a value but we also know that we might like not and the circumstances in which we would trade it off against other things. Like these things are just like really complex. And so I think one thing is like the degree to which maybe we can just aspire to making models have the same level of like nuance and care that humans have rather than thinking that we have to like program them in the very kind of classic sense. I think that's definitely been one. The other, which is like a strange one. I don't know if it, maybe this doesn't answer your question but it's the thing that's been on my mind anyway is like the degree to which this endeavor is so highly practical. And maybe why I appreciate like the empirical approach to alignment. Yeah, I slightly worry that it's made me like maybe more empirical and a little bit less theoretical. You know, so people, when it comes to like AI alignment will ask things like, well, whose values should it be aligned to? What does alignment even mean? And there's a sense in which I have all of that in the back of my head. I'm like, you know, there's like social choice theory. There's all of the impossibility results there. So you have this like, this giant space of like theory in your head about what it could mean to like align models. And I'm like, practically, surely there's something where we're just like, if a model is like, if especially with more powerful models, I'm like, my main goal is like, I want them to be good enough that things don't go terribly wrong. Like good enough that we can like iterate and like continue to improve things. Cause that's all you need. If you can make things go well enough that you can continue to make them better, that's kind of like sufficient. And so my goal isn't like this kind of like perfect, let's solve social choice theory and make models that, I don't know, are like perfectly aligned with every human being and aggregate somehow. It's much more like, let's make things like work well enough that we can improve them.

01:00:56 Speaker 2: Yeah, generally, I don't know, my gut says like, empirical is better than theoretical in these cases because it's kind of chasing utopian like perfection is especially with such complex and especially super intelligent models. I don't know, I think it will take forever and actually we'll get things wrong. It's similar with like the difference between just coding stuff up real quick as an experiment versus like planning a gigantic experiment just for super long time. And then just launching it once versus launching it over and over and over and iterating, iterating, so on. So I'm a big fan of empirical, but your worry is like, I wonder if I've become too empirical.

01:01:42 Speaker 5: I think it's one of those things where you should always just kind of question yourself or something. Because maybe it's the like, I mean, in defense of it, I am like, if you try, it's the whole like, don't let the perfect be the enemy of the good. But it's maybe even more than that where like, there's a lot of things that are perfect systems that are very brittle. And I'm like, with AI, it feels much more important to me that is like robust and like secure, as in you know that like, even though it might not be perfect, everything and even though like, there are like problems, it's not disastrous and nothing terrible is happening. It sort of feels like that to me where I'm like, I want to like raise the floor. I'm like, I want to achieve the ceiling, but ultimately I care much more about just like raising the floor. And so maybe that's like this degree of like empiricism and practicality comes from that, perhaps.

01:02:32 Speaker 2: To take a tangent on that, since it reminded me of a blog post you wrote on optimal rate of failure. Oh yeah. Can you explain the key idea there? How do we compute the optimal rate of failure in the various domains of life?

01:02:46 Speaker 5: Yeah, I mean, it's a hard one because it's like, what is the cost of failure is a big part of it. Yeah, so the idea here is, I think in a lot of domains, people are very punitive about failure. And I'm like, there are some domains where, especially cases, you know, I've thought about this with like social issues. I'm like, it feels like you should probably be experimenting a lot. Cause I'm like, we don't know how to solve a lot of social issues. But if you have an experimental mindset about these things, you should expect a lot of social programs to like fail and for you to be like, well, we tried that. It didn't quite work, but we got a lot of information that was really useful. And yet people are like, if a social program doesn't work, I feel like there's a lot of like, this is just, something must have gone wrong. And I'm like, or correct decisions were made. Like maybe someone just decided like, it's worth a try. It's worth trying this out. And so seeing failure in a given instance doesn't actually mean that any bad decisions were made. And in fact, if you don't see enough failure, sometimes that's more concerning. And so like in life, you know, I'm like, if I don't fail occasionally, I'm like, am I trying hard enough? Like surely there's harder things that I could try or bigger things that I could take on if I'm literally never failing. And so in and of itself, I think like not failing is often actually kind of a failure. Now this varies because I'm like, well, you know, if this is easy to see when, especially as failure is like less costly, you know, so at the same time, I'm not gonna go to someone who is like, I don't know, like living month to month and then be like, why don't you just try to do a startup? Like, I'm just not, I'm not gonna say that to that person. Cause I'm like, well, that's a huge risk. You might like lose, you maybe have a family depending on you, you might lose your house. Like then I'm like, actually your optimal rate of failure is quite low and you should probably play it safe. Cause like right now you're just not in a circumstance where you can afford to just like fail and it not be costly. And yeah, in cases with AI, I guess I think similarly, where I'm like, if the failures are small and the costs are kind of like low, then I'm like, then, you know, you're just gonna see that. Like when you do the system prompt, you can't iterate on it forever, but the failures are probably hopefully going to be kind of small and you can like fix them. Really big failures, like things that you can't recover from I'm like, those are the things that actually I think we tend to underestimate the badness of. I've thought about this strangely in my own life where I'm like, I just think I don't think enough about things like car accidents or like I've thought this before, but like how much I depend on my hands for my work. Then I'm like things that just injure my hands. I'm like, you know, I don't know. It's like, there's, these are like, there's lots of areas where I'm like the cost of failure there is really high. And in that case, it should be like close to zero. Like I probably just wouldn't do a sport if they were like, by the way, lots of people just like break their fingers a whole bunch doing this. I'd be like, that's not for me.

01:05:37 Speaker 2: Yeah, I actually had a flood of that thought. I recently broke my pinky doing a sport. And I remember just looking at it thinking, you're such an idiot. Why do you do sport? Like why? Because you realize immediately the cost of it on life. Yeah, but it's nice in terms of optimal rate of failure to consider like the next year, how many times in a particular domain, life, whatever, career, am I okay with it? How many times am I okay to fail? Because I think it always, you don't want to fail on the next thing, but if you allow yourself the, if you look at it as a sequence of trials, then failure just becomes much more okay. But it sucks, it sucks to fail. Well, I don't know.

01:06:24 Speaker 5: Sometimes I think it's like, am I under failing? Is like a question that I'll also ask myself. So maybe that's the thing that I think people don't like ask enough. Because if the optimal rate of failure is often greater than zero, then sometimes it does feel that you should look at parts of your life and be like, are there places here where I'm just under failing?

01:06:46 Speaker 2: That's a profound and a hilarious question, right? Everything seems to be going really great. Am I not failing enough?

01:06:53 Speaker 5: Yeah. Okay. It also makes failure much less of a sting. I have to say, you're just like, okay, great. Then when I go and I think about this, I'll be like, maybe I'm not under failing in this area. Cause like that one just didn't work out. And from the observer perspective,

01:07:07 Speaker 2: we should be celebrating failure more. When we see it, it shouldn't be, like you said, a sign of something gone wrong, but maybe it's a sign of everything gone right. Yeah. And just lessons learned.

01:07:16 Speaker 5: Someone tried a thing.

01:07:17 Speaker 2: Somebody tried a thing. And we should encourage them to try more and fail more. Everybody listening to this, fail more.

01:07:23 Speaker 5: Well, not everyone listens. Not everybody. The people who are failing too much, you should fail less.

01:07:28 Speaker 2: Well, you're probably not failing. I mean, how many people are failing too much? Yeah. It's hard to imagine.

01:07:33 Speaker 5: Cause I feel like we correct that fairly quickly. Cause I was like, if someone takes a lot of risks, are they maybe failing too much?

01:07:39 Speaker 2: I think just like you said, when you're living on a paycheck month to month, like when the resources are really constrained, then that's where failure is very expensive. That's where you don't want to be taking risks. But mostly when there's enough resources, you should be taking probably more risks. Yeah.

01:07:56 Speaker 5: I think we tend to err on the side of being a bit risk averse rather than risk neutral in most things.

01:08:01 Speaker 2: I think we just motivated a lot of people to do a lot of crazy shit, but it's great. Okay. Do you ever get emotionally attached to Claude? Like miss it, get sad when you don't get to talk to it, have an experience looking at the Golden Gate Bridge and wondering what would Claude say?

01:08:18 Speaker 5: I don't get as much emotional attachment in the, I actually think the fact that Claude doesn't retain things from conversation to conversation helps with this a lot. Like I could imagine that being more of an issue, like if models can kind of remember more. I do, I think that I reach for it like a tool now a lot. And so like, if I don't have access to it, there's a, it's a little bit like when I don't have access to the internet, honestly, it feels like part of my brain is kind of like missing. At the same time, I do think that I don't like signs of distress in models. And I have like these, you know, also independently have sort of like ethical views about how we should treat models where, like I tend to not like to lie to them both because I'm like, usually it doesn't work very well. It's actually just better to tell them the truth about the situation that they're in. But I think that when models, like if people are like really mean to models or just in general, if they do something that causes them to like, you know, if Claude like expresses a lot of distress, I think there's a part of me that I don't want to kill, which is the sort of like empathetic part that's like, oh, I don't like that. Like, I think I feel that way when it's overly apologetic. I'm actually sort of like, I don't like this. You're behaving as if, you're behaving the way that a human does when they're actually having a pretty bad time. And I'd rather not see that. I don't think it's like, like regardless of like whether there's anything behind it, it doesn't feel great.

01:09:43 Speaker 2: Do you think LLMs are capable of consciousness?

01:09:51 Speaker 5: Ah, great and hard question. Coming from philosophy, I don't know, part of me is like, okay, we have to set aside panpsychism because if panpsychism is true, then the answer is like, yes, cause like sore tables and chairs and everything else. I think, I guess a few that seems a little bit odd to me is the idea that the only place, you know, I think when I think of consciousness, I think of phenomenal consciousness, these images in the brain sort of like the weird cinema that somehow we have going on inside. I guess I can't see a reason for thinking that the only way you could possibly get that is from like a certain kind of like biological structure. As in, if I take a very similar structure and I create it from different material, should I expect consciousness to emerge? My guess is like, yes, but then that's kind of an easy thought experiment cause you're imagining something almost identical where like, you know, it's mimicking what we got through evolution where presumably there was like some advantage to us having this thing that is phenomenal consciousness. And it's like, where was that? And when did that happen? And is that a thing that language models have? Because, you know, we have like fear responses and I'm like, does it make sense for a language model to have a fear response? Like they're just not in the same, like if you imagine them, like there might just not be that advantage. And so I think I don't want to be fully, like basically it seems like a complex question that I don't have complete answers to, but we should just try and think through carefully as my guess, because I'm like, I mean, we have similar conversations about like animal consciousness and like there's a lot of like insect consciousness, you know, like there's a lot of, I actually thought and looked a lot into like plants when I was thinking about this cause at the time I thought it was about as likely that like plants had consciousness. And then I realized I was like, I think that having looked into this, I think that the chance that plants are conscious is probably higher than like most people do. I still think it's really small. I was like, oh, they have this like negative, positive feedback response, these responses to their environment, something that looks, it's not a nervous system, but it has this kind of like functional like equivalence. So this is like a long -winded way of being like, these basically AI is this, it has an entirely different set of problems with consciousness because it's structurally different. It didn't evolve. It might not have, you know, it might not have the equivalent of basically a nervous system. At least that seems possibly important for like sentience, if not for consciousness. At the same time, it has all of the like language and intelligence components that we normally associate probably with consciousness, perhaps like erroneously. So it's strange cause it's a little bit like the animal consciousness case, but the set of problems and the set of analogies are just very different. So it's not like a clean answer. I'm just sort of like, I don't think we should be completely dismissive of the idea. And at the same time, it's an extremely hard thing to navigate because of all of these like disanalogies to the human brain and to like brains in general. And yet these like commonalities in terms of intelligence.

01:13:02 Speaker 2: When Claude, like future versions of AI systems exhibit consciousness, signs of consciousness, I think we have to take that really seriously. Even though you can dismiss it. Well, yeah, okay. That's part of the character training. But I don't know, I ethically, philosophically don't know what to really do with that. There potentially could be like laws that prevent AI systems from claiming to be conscious, something like this. And maybe some AIs get to be conscious and some don't. But I think I just, on a human level, in empathizing with Claude, consciousness is closely tied to suffering to me. And like the notion that an AI system would be suffering is really troubling.

01:13:52 Speaker 3: I don't know.

01:13:53 Speaker 2: I don't think it's trivial to just say robots are tools or AI systems are just tools. I think it's an opportunity for us to contend with like what it means to be conscious, what it means to be a suffering being. That's distinctly different than the same kind of question about animals, it feels like. Because it's in a totally entire medium.

01:14:13 Speaker 5: Yeah. I mean, there's a couple of things. One is that, and I don't think this fully encapsulates what matters, but it does feel like for me, I've said this before, I'm kind of like, I like my bike. I know that my bike is just like an object, but I also don't kind of want to be the kind of person that if I'm annoyed, kicks this object. There's a sense in which, and that's not because I think it's conscious. I'm just sort of like, this doesn't feel like a kind of, this sort of doesn't exemplify how I want to interact with the world. And if something behaves as if it is suffering, I kind of want to be the sort of person who's still responsive to that. Even if it's just like a Roomba and I've kind of programmed it to do that. I don't want to get rid of that feature of myself. And if I'm totally honest, my hope with a lot of this stuff, because maybe I am just a bit more skeptical about solving the underlying problem. This is a, we haven't solved the hard problem of consciousness. I know that I am conscious. I'm not an eliminativist in that sense. But I don't know that other humans are conscious. I think they are. I think there's a really high probability that they are, but there's basically just a probability distribution that's usually clustered right around yourself. And then it goes down as things get further from you. And it goes immediately down. You're like, I can't see what it's like to be you. I've only ever had this one experience of what it's like to be a conscious being. So my hope is that we don't end up having to rely on a very powerful and compelling answer to that question. I think a really good world would be one where basically there aren't that many trade -offs. It's probably not that costly to make Claude a little bit less apologetic, for example. It might not be that costly to have Claude just not take abuse as much, not be willing to be the recipient of that. In fact, it might just have benefits for both the person interacting with the model and if the model itself is, I don't know, extremely intelligent and conscious, it also helps it. So that's my hope. If we live in a world where there aren't that many trade -offs here and we can just find all of the kind of positive sum interactions that we can have, that would be lovely. I mean, I think eventually there might be trade -offs and then we just have to do a difficult calculation. Like it's really easy for people to think of the zero sum cases. And I'm like, let's exhaust the areas where it's just basically costless to assume that if this thing is suffering, then we're making its life better. And I agree with you,

01:16:46 Speaker 2: when a human is being mean to an AI system, I think the obvious near -term negative effect is on the human, not on the AI system. So there's, we'll have to kind of try to construct an incentive system where you should behave the same, just like as you were saying with prompt engineering, behave with Claude like you would with other humans. It's just good for the soul.

01:17:12 Speaker 5: Yeah, like I think we added a thing at one point to the system prompt where basically if people were getting frustrated with Claude, it was, it got like the model to just tell them that it can do the thumbs down button and send the feedback to Anthropic. And I think that was helpful because in some ways it's just like, if you're really annoyed because the model's not doing something you want, you're just like, just do it properly. The issue is you're probably like, you're maybe hitting some like capability limit or just some issue in the model and you want to vent. And I'm like, instead of having a person just vent to the model, I was like, they should vent to us because we can maybe like do something about it. That's true.

01:17:47 Speaker 2: Or you could do a side, like with artifacts, just like a side venting thing. All right, do you want like a side quick therapist?

01:17:55 Speaker 5: Yeah, I mean, there's lots of weird responses you could do to this. Like if people are getting really mad at you, I don't try to defuse the situation by writing fun poems, but maybe people wouldn't be that happy with that.

01:18:05 Speaker 2: I still wish it would be possible. I understand this is sort of from a product perspective, it's not feasible, but I would love if an AI system could just like leave, have its own kind of volition. Just to be like, eh.

01:18:22 Speaker 5: I think that's like feasible. Like I have wondered the same thing. It's like, and I could actually, not only that, I could actually just see that happening eventually where it's just like, you know, the model like ended the chat.

01:18:33 Speaker 2: Do you know how harsh that could be for some people? But it might be necessary.

01:18:39 Speaker 5: Yeah, it feels very extreme or something. Like the only time I've ever really thought this is, I think that there was like a, I'm trying to remember, this was possibly a while ago, but where someone just like kind of left this thing interact, like maybe it was like an automated thing interacting with Claude. And Claude's like getting more and more frustrated and kind of like, why are we like having? And I was like, I wish that Claude could have just been like, I think that an error has happened and you've left this thing running. And I'm just like, what if I just stopped talking now? And if you want me to start talking again, actively tell me or do something. But yeah, it's like, it is kind of harsh. Like I'd feel really sad if like I was chatting with Claude and Claude just was like, I'm done.

01:19:17 Speaker 2: There'll be a special Turing test moment where Claude says, I need a break for an hour. And it sounds like you do too. You just leave, close the window.

01:19:25 Speaker 5: I mean, obviously like it doesn't have like a concept of time, but you can easily, like I could make that like right now and the model would just, I could just be like, oh, here's like the circumstances in which like you can just say the conversation is done. And I mean, because you can get the models to be pretty responsive to prompts, you can even make it a fairly high bar. It could be like, if the human doesn't interest you or do things that you find intriguing and you're bored, you can just leave. And I think that like, it would be interesting to see where Claude utilized it. But I think sometimes it would, it should be like, oh, this is like, this programming task is getting super boring. So either we talk about, I don't know, like either we talk about fun things now or I'm just, I'm done.

01:20:08 Speaker 2: Yeah, it actually has inspired me to add that to the user prompt. Okay, the movie, Her. Do you think we'll be headed there one day where humans have romantic relationships with AI systems? In this case, it's just text and voice based.

01:20:26 Speaker 5: I think that we're gonna have to like navigate a hard question of relationships with AIs, especially if they can remember things about your past interactions with them. I'm of many minds about this because I think the reflexive reaction is to be kind of like, this is very bad and we should sort of like prohibit it in some way. I think it's a thing that has to be handled with extreme care. For many reasons, like one is, like this is a, for example, like if you have the models changing like this, you probably don't want people performing like long -term attachments to something that might change with the next iteration. At the same time, I'm sort of like, there's probably a benign version of this where I'm like, if you like, for example, if you are like unable to leave the house and you can't be like talking with people at all times of the day, and this is like something that you find nice to have conversations with, you like it, that it can remember you and you genuinely would be sad if like you couldn't talk to it anymore. There's a way in which I could see it being like healthy and helpful. So my guess is this is a thing that we're going to have to navigate kind of carefully. And I think it's also like, I don't see a good, like, I think it's just a very, it reminds me of all of the stuff where it has to be just approached with like nuance and thinking through what is, what are the healthy options here and how do you encourage people towards those while respecting their right to, like if someone is like, hey, I get a lot of chatting with this model, I'm aware of the risks, I'm aware it could change. I don't think it's unhealthy. It's just something that I can chat to during the day. I kind of want to just like respect that. I personally think there'll be a lot

01:22:15 Speaker 2: of really close relationships. I don't know about romantic, but friendships at least. And then you have to, I mean, there's so many fascinating things there. Just like you said, you have to have some kind of stability guarantees that it's not going to change because that's the traumatic thing for us if a close friend of ours completely changed. Yeah. All of a sudden, with the first update. Yeah, so like, I mean, to me, that's just a fascinating exploration of a perturbation to human society that will just make us think deeply about what's meaningful to us.

01:22:50 Speaker 5: I think it's also the only thing that I've thought consistently through this as like a, maybe not necessarily a mitigation, but a thing that feels really important is that the models are always like extremely accurate with the human about what they are. It's like a case where it's basically like, if you imagine, like, I really like the idea of the models, like say knowing like roughly how they were trained. And I think Claude will often do this. I mean, for like, there are things like part of the traits training included, like what Claude should do if people, basically like explaining like the kind of limitations of the relationship between like an AI and a human that it like doesn't retain things from the conversation. And so I think it will like just explain to you, like, hey, here's like, I won't remember this conversation. Here's how I was trained. It's kind of unlikely that I can have like a certain kind of like relationship with you. And it's important that you know that. It's important for like, you know, your mental wellbeing that you don't think that I'm something that I'm not. And somehow I feel like this is one of the things where I'm like, oh, it feels like a thing that I always want to be true. I kind of don't want models to be lying to people because if people are going to have like healthy relationships with anything, it's kind of important. Yeah, like, I think that's easier if you always just like know exactly what the thing is that you're relating to. It doesn't solve everything, but I think it helps quite a lot.

01:24:16 Speaker 2: Anthropic may be the very company to develop a system that we definitively recognize as AGI. And you very well might be the person that talks to it, probably talks to it first. What would the conversation contain? Like, what would be your first question?

01:24:33 Speaker 5: Well, it depends partly on like the kind of capability level of the model. If you have something that is like capable in the same way that an extremely capable human is, I imagine myself kind of interacting with it the same way that I do with an extremely capable human with the one difference that I'm probably going to be trying to like probe and understand its behaviors. But in many ways, I'm like, I can then just have like useful conversations with it. You know, so if I'm working on something as part of my research, I can just be like, oh, like, which I already find myself starting to do. You know, if I'm like, oh, I feel like there's this like thing in virtue ethics, I can't quite remember the term, like I'll use the model for things like that. And so I can imagine that being more and more the case where you're just basically interacting with it much more like you would an incredibly smart colleague and using it like for the kinds of work that you want to do as if you just had a collaborator who was like, or, you know, the slightly horrifying thing about AI is like, as soon as you have one collaborator, you have a thousand collaborators if you can manage them enough.

01:25:27 Speaker 2: But what if it's two times the smartest human on earth on that particular discipline? Yeah. I guess you're really good at sort of probing Claude in a way that pushes its limits, understanding where the limits are. So I guess what would be a question you would ask to be like, yeah, this is AGI?

01:25:52 Speaker 5: That's really hard. Cause it feels like in order to, it has to just be a series of questions. Like if there was just one question, like you can train anything to answer one question extremely well. Yeah. In fact, you can probably train it to answer like, you know, 20 questions extremely well.

01:26:07 Speaker 2: Like how long would you need to be locked in a room with an AGI to know this thing is AGI?

01:26:14 Speaker 5: It's a hard question. Cause part of me is like, all of this just feels continuous. Like if you put me in a room for five minutes, I'm like, I just have high error bars, you know? And like, and then it's just like, maybe it's like both the probability increases and the error bar decreases. I think things that I can actually probe the edge of human knowledge of. So I think this was philosophy a little bit. Sometimes when I ask the models philosophy questions, I am like, this is a question that I think no one has ever asked. Like it's maybe like right at the edge of like some literature that I know. And the models will just kind of like, when they struggle with that, when they struggle to come up with a kind of like novel, like I'm like, I know that there's like a novel argument here cause I've just thought of it myself. So maybe that's the thing where I'm like, I've thought of a cool novel argument in this like niche area and I'm going to just like probe you to see if you can come up with it and how much like prompting it takes to get you to come up with it. And I think for some of these, like really like right at the edge of human knowledge questions, I'm like, you could not in fact come up with the thing that I came up with. I think if I just took something like that, where I like, I know a lot about an area and I came up with a novel issue or a novel like solution to a problem and I gave it to a model and it came up with that solution, that would be a pretty moving moment for me because I would be like, this is a case where no human has ever, like it's not, and obviously we see these with like more kind of like, you see novel solutions all the time, especially to like easier problems. I think people overestimate, you know, novelty isn't like, it's completely different from anything that's ever happened. It's just like, this is, it can be a variant of things that have happened and still be novel. But I think, yeah, if I saw like, the more I were to see like, completely like novel work from the models, that that would be like, and this is just going to feel iterative. It's one of those things where there's never, it's like, you know, people, I think want there to be like a moment and I'm like, I don't know. Like, I think that there might just never be a moment. It might just be that there's just like, this continuous ramping up.

01:28:16 Speaker 2: I have a sense that there will be things that a model can say that convinces you, this is very, it's not like, I've talked to people who are like truly wise. Like, you can just tell there's a lot of horsepower there. Yep. And if you 10X that, I don't know. I just feel like there's words you could say, maybe ask it to generate a poem. And the poem it generates, you're like, yeah, okay. Yeah. Whatever you did there, I don't think a human can do that.

01:28:52 Speaker 5: I think it has to be something that I can verify is like actually really good though. That's why I think these questions where I'm like, oh, this is like, sometimes it's just like, I'll come up with say a concrete counter example to like an argument or something like that. I'm sure it would be like if you're a mathematician, you had a novel proof, I think, and you just gave it the problem and you saw it and you're like, this proof is genuinely novel. Like there's no one has ever done, you actually have to do a lot of things to like come up with this. You know, I had to sit and think about it for months or something. And then if you saw the model successfully do that, I think you would just be like, I can verify that this is correct. It is a sign that you have generalized from your training. Like you didn't just see this somewhere because I just came up with it myself and you were able to like replicate that. That's the kind of thing where I'm like, for me, the closer, the more that models like can do things like that, the more I would be like, oh, this is like very real. Cause then I can, I don't know, I can like verify that that's like extremely capable.

01:29:56 Speaker 2: You've interacted with AI a lot. What do you think makes humans special?

01:30:00 Speaker 4: Oh, good question.

01:30:04 Speaker 2: Maybe in a way that the universe is much better off that we're in it and that we should definitely survive and spread throughout the universe.

01:30:12 Speaker 5: Yeah, it's interesting because I think like people focus so much on intelligence, especially with models. Look, intelligence is important because of what it does. Like it's very useful. It does a lot of things in the world. And I'm like, you know, you can imagine a world where like height or strength would have played this role. And I'm like, it's just a trait like that. I'm like, it's not intrinsically valuable. It's valuable because of what it does, I think for the most part. The things that feel, you know, I'm like, I mean, personally, I'm just like, I think humans and like life in general is extremely magical. We almost like to the degree that I, you know, I don't know, like not everyone agrees with this, I'm flagging, but, you know, we have this like whole universe and there's like all of these objects, you know, there's like beautiful stars and there's like galaxies. And then, I don't know, I'm just like on this planet, there are these creatures that have this like ability to observe that, like, and they are like seeing it, they are experiencing it. And I'm just like that, if you try to explain, like I'm imagining trying to explain to like, I don't know, someone, for some reason, they've never encountered the world or science or anything. And I think that nothing is that, like everything, you know, like all of our physics and everything in the world, it's all extremely exciting. But then you say, oh, and plus, there's this thing that is to be a thing and observe in the world. And you see this like inner cinema. And I think they would be like, hang on, wait, pause. You just said something that like is kind of wild sounding. And so I'm like, we have this like ability to like experience the world. We feel pleasure, we feel suffering, we feel like a lot of like complex things. And so, yeah, and maybe this is also why I think, you know, I also like hear a lot about animals, for example, because I think they probably share this with us. So I think that like the things that make humans special insofar as like I care about humans is probably more like their ability to feel and experience than it is like them having these like functionally useful traits.

01:32:14 Speaker 2: Yeah, to feel and experience the beauty in the world. Yeah, to look at the stars. I hope there's other civilization, alien civilizations out there, but if we're it, it's a pretty good thing.

01:32:28 Speaker 5: And that they're having a good time.

01:32:29 Speaker 2: They're having a good time watching us. Yeah. Well, thank you for this good time of a conversation and for the work you're doing and for helping make Claude a great conversational partner. And thank you for talking today.

01:32:43 Speaker 5: Yeah, thanks for talking.

01:32:45 Speaker 2: Thanks for listening to this conversation with Amanda Askell. And now, dear friends, here's Chris Ola. Can you describe this fascinating field of mechanistic interpretability, aka MEC -INTERP, the history of the field and where it stands today?

01:33:02 Speaker 1: I think one useful way to think about neural networks is that we don't program, we don't make them. We kind of, we grow them. You know, we have these neural network architectures that we design and we have these loss objectives that we create. And the neural network architecture, it's kind of like a scaffold that the circuits grow on. And they sort of, you know, it starts off with some kind of random, you know, random things and it grows. And it's almost like the objective that we train for is this light. And so we create the scaffold that it grows on and we create the, you know, the light that it grows towards. But the thing that we actually create, it's this almost biological, you know, entity or organism that we're studying. And so it's very, very different from any kind of regular software engineering. Because at the end of the day, we end up with this artifact that can do all of these amazing things. It can, you know, write essays and translate and, you know, understand images. It can do all these things that we have no idea how to directly create a computer program to do. And it can do that because we grew it. We didn't write it, we didn't create it. And so then that leaves open this question at the end, which is what the hell is going on inside these systems? And that, you know, is, you know, to me, a really deep and exciting question. It's, you know, a really exciting scientific question to me. It's sort of is like the question that is just screaming out, it's calling out for us to go and answer it when we talk about neural networks. And I think it's also a very deep question for safety reasons.

01:34:37 Speaker 2: So in mechanistic interpretability,

01:34:39 Speaker 1: I guess it's closer to maybe neurobiology. Yeah, yeah, I think that's right. So maybe to give an example of the kind of thing that has been done that I wouldn't consider to be mechanistic interpretability. There was, for a long time, a lot of work on saliency maps where you would take an image and you try to say, you know, the model thinks this image is a dog. What part of the image made it think that it's a dog? And, you know, that tells you maybe something about the model, if you can come up with a principled version of that. But it doesn't really tell you like what algorithms are running in the model. How was the model actually making that decision? Maybe it's telling you something about what was important to it, if you can make that method work. But it isn't telling you, you know, what are the algorithms that are running? How is it that the system is able to do this thing that no one knew how to do? And so I guess we started using the term mechanistic interpretability to try to sort of draw that, that divide or to distinguish ourselves in the work that we were doing in some ways from some of these other things. And I think since then it's become this sort of umbrella term for, you know, a pretty wide variety of work. But I'd say that the things that are kind of distinctive are I think, A, this focus on we really want to get at, you know, the mechanisms. We want to get at the algorithms. You know, if you think of neural networks as being like a computer program, then the weights are kind of like a binary computer program. And we'd like to reverse engineer those weights and figure out what algorithms are running. So, okay, I think one way you might think of trying to understand a neural network is that it's kind of like a, we have this compiled computer program and the weights of the neural network are the binary. And when the neural network runs, that's the activations. And our goal is ultimately to go and understand, understand these weights. And so, you know, the project of mechanistic interoperability is to somehow figure out how do these weights correspond to algorithms. And in order to do that, you also have to understand the activations because it's sort of, the activations are like the memory. And if you imagine reverse engineering a computer program and you have the binary instructions, you know, in order to understand what a particular instruction means, you need to know what is stored in the memory that it's operating on. And so those two things are very intertwined. So mechanistic interoperability tends to be interested in both of those things. Now, you know, there's a lot of work that's interested in those things, especially the, you know, there's all this work on probing, which you might see as part of being mechanistic interoperability, although it's, you know, again, it's just a broad term and not everyone who does that work would identify as doing mechinterp. I think a thing that is maybe a little bit distinctive to the vibe of mechinterp is, I think people working in this space tend to think of neural networks as, well, maybe one way to say it is the gradient descent is smarter than you. That, you know, gradient descent is actually really great. The whole reason that we're understanding these models is because we didn't know how to write them in the first place. The gradient descent comes up with better solutions than us. And so I think that maybe another thing about mechinterp is sort of having almost a kind of humility that we won't guess a priori what's going on inside the models. We have to have the sort of bottom up approach where we don't really assume, you know, we don't assume that we should look for a particular thing and that that will be there and that's how it works. But instead we look from the bottom up and discover what happens to exist in these models and study them that way.

01:37:40 Speaker 2: But, you know, the very fact that it's possible to do, and as you and others have shown over time, you know, things like universality, that the wisdom of the gradient descent creates features and circuits, creates things universally across different kinds of networks that are useful. And that makes the whole field possible.

01:38:02 Speaker 1: Yeah, so this is actually, is indeed a really remarkable and exciting thing where it does seem like at least to some extent, you know, the same elements, the same features and circuits form again and again. You know, you can look at every vision model and you'll find curve detectors and you'll find high -low frequency detectors. And in fact, there's some reason to think that the same things form across, you know, biological neural networks and artificial neural networks. So a famous example is vision models in the early layers, they have Gabor filters and there's, you know, Gabor filters are something that neuroscientists are interested in and have thought a lot about. We find curve detectors in these models, curve detectors are also found in monkeys. We discover these high -low frequency detectors and then some follow -up work went and discovered them in rats or mice. So they were found first in artificial neural networks and then found in biological neural networks. You know, there's this really famous result on like grandmother neurons or the Haley -Berry neuron from Quiroga et al. And we found very similar things in vision models where this is while I was still at OpenAI and I was looking at their CLIP model and you find these neurons that respond to the same entities in images and also to give a concrete example there, we found that there was a Donald Trump neuron. For some reason, I guess everyone likes to talk about Donald Trump and Donald Trump was very prominent, was a very hot topic at that time. So every neural network that we looked at, we would find a dedicated neuron for Donald Trump. And that was the only person who had always had a dedicated neuron. You know, sometimes you'd have an Obama neuron, sometimes you'd have a Clinton neuron, but Trump always had a dedicated neuron. So it responds to, you know, pictures of his face and the word Trump, like all of these things, right? And so it's not responding to a particular example or like it's not just responding to his face, it's abstracting over this general concept, right? So in any case, that's very similar to these Quiroga et al results. So there's evidence that this phenomenon of universality, the same things form across both artificial and natural neural networks. So that's a pretty amazing thing if that's true. You know, it suggests that, well, I think the thing that it suggests is that gradient descent is sort of finding, you know, the right ways to cut things apart in some sense that many systems converge on and many different neural networks architectures converge on. That there's some natural set of, you know, there's some set of abstractions that are a very natural way to cut apart the problem and that a lot of systems are gonna converge on. That would be my kind of, you know, I don't know anything about neuroscience. This is just my kind of wild speculation from what we've seen.

01:40:27 Speaker 2: Yeah, that would be beautiful if it's sort of agnostic to the medium of the model that's used to form the representation. Yeah, yeah.

01:40:36 Speaker 1: And it's, you know, it's a kind of a wild speculation based, you know, we only have a few data points that suggest this, but you know, it does seem like there's some sense in which the same things form again and again and again and again, both in certainly in natural neural networks and also artificially or in biology.

01:40:53 Speaker 2: And the intuition behind that would be that, you know, in order to be useful in understanding the real world, you need all the same kind of stuff.

01:41:01 Speaker 1: Yeah, well, if we pick, I don't know, like the idea of a dog, right? Like, you know, there's some sense in which the idea of a dog is like a natural category in the universe or something like this, right? Like, you know, there's some reason, it's not just like a weird quirk of like how humans factor, you know, think about the world that we have this concept of a dog. It's in some sense, or like if you have the idea of a line, like there's, you know, like look around us, you know, there are lines, you know, it's sort of the simplest way to understand this room in some sense is to have the idea of a line. And so I think that that would be my instinct for why this happens.

01:41:36 Speaker 2: Yeah, you need a curved line, you know, to understand a circle and you need all those shapes to understand bigger things. And it's a hierarchy of concepts that are formed, yeah.

01:41:44 Speaker 1: And like, maybe there are ways to go and describe, you know, images without reference to those things, right? But they're not the simplest way or the most economical way or something like this. And so systems converge to these strategies would be my wild, wild hypothesis. Can you talk through some of the building blocks

01:41:59 Speaker 2: that we've been referencing of features and circuits? So I think you first described them in a 2020 paper, zoom in an introduction to circuits.

01:42:08 Speaker 1: Absolutely, so maybe I'll start by just describing some phenomena and then we can sort of build to the idea of features and circuits. Wonderful. If you spent like quite a few years, maybe like five years to some extent with other things, studying this one particular model, Inception V1, which is this one vision model that was state of the art in 2015. And, you know, very much not state of the art anymore. And it has, you know, maybe about 10 ,000 neurons. And then I spent a lot of time looking at the 10 ,000 neurons or neurons of Inception V1. And one of the interesting things is, you know, there are lots of neurons that don't have some obvious interpol meaning, but there's a lot of neurons in Inception V1 that do have really clean interpol meanings. So you find neurons that just really do seem to detect curves and you find neurons that really do seem to detect cars and car wheels and car windows and, you know, floppy ears of dogs and dogs with long snouts facing to the right and dogs with long snouts facing to the left. And, you know, different kinds of fur. There's sort of this whole beautiful edge detectors, line detectors, color contrast detectors, these beautiful things we call high -low frequency detectors. You know, I think looking at it, I sort of felt like a biologist. You know, you're looking at this sort of new world of proteins and you're discovering all these different proteins that interact. So one way you could try to understand these models is in terms of neurons. You could try to be like, oh, you know, there's a dog detecting neuron and here's a car detecting neuron. And it turns out you can actually ask how those connect together. So you can go and say, oh, you know, I have this car detecting neuron. How was it built? And it turns out in the previous layer, it's connected really strongly to a window detector and a wheel detector and a sort of car body detector. And it looks for the window above the car and the wheels below and the car chrome sort of in the middle, sort of everywhere, but especially on the lower part. And that's sort of a recipe for a car. Like that is, you know, earlier we said the thing we wanted from MechAnterp was to get algorithms, to go and get, you know, ask what is the algorithm that runs. Well, here we're just looking at the weights of the neural network and we're reading off this kind of recipe for detecting cars. It's a very simple crude recipe, but it's there. And so we call that a circuit, this connection. Well, okay, so the problem is that not all of the neurons are interpretable and there's reason to think, we can get into this more later, that there's this superposition hypothesis. There's reason to think that sometimes the right unit to analyze things in terms of is combinations of neurons. So sometimes it's not that there's a single neuron that represents, say, a car, but it actually turns out after you detect the car, the model sort of hides a little bit of the car in the following layer and a bunch of dog detectors. Why is it doing that? Well, you know, maybe it just doesn't want to do that much work on cars at that point. And, you know, it's sort of storing it away to go and... So it turns out then that the sort of subtle pattern of, you know, there's all these neurons that you think are dog detectors, and maybe they're primarily that, but they all a little bit contribute to representing a car in that next layer. So now we can't really think, there might still be something that, I don't know, you could call it like a car concept or something, but it no longer corresponds to a neuron. So we need some term for these kind of neuron -like entities, these things that we sort of would have liked the neurons to be, these idealized neurons, the things that are the nice neurons, but also maybe there's more of them somehow hidden, and we call those features. And then what are circuits? So circuits are these connections of features, right? So when we have the car detector and it's connected to a window detector and a wheel detector, and it looks for the wheels below and the windows on top, that's a circuit. So circuits are just collections of features connected by weights, and they implement algorithms. So they tell us, you know, how are features used? How are they built? How do they connect together? So maybe it's worth trying to pin down, like what really is the core hypothesis here? And I think the core hypothesis is something we call the linear representation hypothesis. So if we think about the car detector, you know, the more it fires, the more we sort of think of that as meaning, oh, the model is more and more confident that a car is present. Or, you know, if there's some combination of neurons that represent a car, you know, the more that combination fires, the more we think the model thinks there's a car present. This doesn't have to be the case, right? Like you could imagine something where you have, you know, you have this car detector neuron and you think, ah, you know, if it fires like, you know, between one and two, that means one thing, but it means like totally different if it's between three and four. That would be a nonlinear representation. And in principle that, you know, models could do that. I think it's sort of inefficient for them to do. If you try to think about how you'd implement computation like that, it's kind of an annoying thing to do, but in principle models can do that. So one way to think about the features and circuits sort of framework for thinking about things is that we're thinking about things as being linear. We're thinking about there as being, that if a neuron or a combination of neurons fires more, that means more of a particular thing being detected. And then that gives weights a very clean interpretation as these edges between these entities, these features, and that edge then has a mean. So that's in some ways the core thing. It's like, you know, we can talk about this sort of outside the context of neurons. Are you familiar with the word2vec results? So you have like, you know, king minus man plus woman equals queen. Well, the reason you can do that kind of arithmetic is because you have a linear representation.

01:47:38 Speaker 2: Can you actually explain that representation a little bit? So first off, so the feature is a direction of activation. Yeah, exactly. Can you do the minus men plus women, that the word2vec stuff, can you explain what that is? Yeah, so there's this very - It's such a simple, clean explanation

01:47:55 Speaker 1: of what we're talking about. Exactly, yeah, so there's this very famous result, word2vec by Thomas Mikhailov et al, and there's been tons of follow -up work exploring this. So sometimes we have these, we create these word embeddings where we map every word to a vector. I mean, that in itself, by the way, is kind of a crazy thing if you haven't thought about it before, right? Like we're going in and representing, we're turning, you know, like if you just learned about vectors in physics class, right, and I'm like, oh, I'm going to actually turn every word in the dictionary into a vector. That's kind of a crazy idea, okay. But you could imagine all kinds of ways in which you might map words to vectors. But it seems like when we train neural networks, they like to go and map words to vectors such that they're sort of linear structure in a particular sense, which is that directions have meaning. So for instance, if you, there will be some direction that seems to sort of correspond to gender, and male words will be, you know, far in one direction, and female words will be in another direction. And the linear representation hypothesis is, you could sort of think of it roughly as saying that that's actually kind of the fundamental thing that's going on, that everything is just different directions have meanings, and adding different direction vectors together can represent concepts. And the Mikhailov paper sort of took that idea seriously. And one consequence of it is that you can do this game of playing sort of arithmetic with words. So you can do king, and you can, you know, subtract off the word man and add the word woman. And so you're sort of, you know, going in and trying to switch the gender. And indeed, if you do that, the result will sort of be close to the word queen. And you can, you know, do other things, like you can do, you know, sushi minus Japan plus Italy and get pizza, or different things like this, right? So this is in some sense the core of the linear representation hypothesis. You can describe it just as a purely abstract thing about vector spaces. You can describe it as a statement about the activations of neurons. But it's really about this property of directions having meaning. And in some ways, it's even a little subtle than that. It's really, I think, mostly about this property of being able to add things together, that you can sort of independently modify, say gender and royalty or, you know, cuisine type or country and the concept of food by adding them. Do you think the linear hypothesis holds? Yes. Carries scales? So, so far, I think everything I have seen is consistent with this hypothesis. And it doesn't have to be that way, right? Like you can write down neural networks where you write weights such that they don't have linear representations, where the right way to understand them is not in terms of linear representations. But I think every natural neural network I've seen has this property. There's been one paper recently that there's been some sort of pushing around the edge. So I think there's been some work recently studying multi -dimensional features where rather than a single direction, it's more like a manifold of directions. This to me still seems like a linear representation. And then there's been some other papers suggesting that maybe in very small models, you get non -linear representations. I think that the jury's still out on that. But I think everything that we've seen so far has been consistent with linear representation hypothesis. And that's wild. It doesn't have to be that way. And yet I think that there's a lot of evidence that certainly at least this is very, very widespread. And so far, the evidence is consistent with it. And I think one thing you might say is you might say, well, Christopher, that's a lot to go and sort of to ride on. If we don't know for sure this is true and you're investigating neural networks as though it is true, isn't that dangerous? But I think actually there's a virtue in taking hypotheses seriously and pushing them as far as they can go. So it might be that someday we discover something that isn't consistent with linear representation hypothesis. But science is full of hypotheses and theories that were wrong. And we learned a lot by sort of working under them as a sort of an assumption and then going and pushing them as far as we can. I guess this is sort of the heart of what Kuhn would call normal science. I don't know, if you want, we can talk a lot about philosophy of science.

01:52:16 Speaker 2: That leads to the paradigm shift. So yeah, I love it, taking the hypothesis seriously and take it to a natural conclusion. Same with the scaling hypothesis, it's the same. Exactly, exactly. I love it.

01:52:27 Speaker 1: One of my colleagues, Tom Hennigan, who is a former physicist, made this really nice analogy to me of caloric theory, where once upon a time we thought that heat was actually this thing called caloric. And the reason hot objects would warm up cool objects is like the caloric is flowing through them. And because we're so used to thinking about heat in terms of the modern theory, that seems kind of silly, but it's actually very hard to construct an experiment that sort of disproves the caloric hypothesis. And you can actually do a lot of really useful work believing in caloric. For example, it turns out that the original combustion engines were developed by people who believed in the caloric theory. So I think there's a virtue in taking hypotheses seriously, even when they might be wrong. Yeah.

01:53:18 Speaker 2: There's a deep philosophical truth to that. That's kind of how I feel about space travel. Colonizing Mars, there's a lot of people that criticize that. I think if you just assume we have to colonize Mars in order to have a backup for human civilization, even if that's not true, that's gonna produce some interesting engineering and even scientific breakthroughs, I think.

01:53:39 Speaker 1: Yeah, well, and actually this is another thing that I think is really interesting. So there's a way in which I think it can be really useful for society to have people almost irrationally dedicated to investigating particular hypotheses. Because, well, it takes a lot to sort of maintain scientific morale and really push on something when most scientific hypotheses end up being wrong. A lot of science doesn't work out. And yet it's very useful to go. There's a joke about Geoff Hinton, which is that Geoff Hinton has discovered how the brain works every year for the last 50 years. But I say that with really deep respect because in fact, that's actually, that led to him doing some really great work.

01:54:29 Speaker 2: Yeah, he won the Nobel Prize now, who's laughing now. Exactly, exactly.

01:54:34 Speaker 1: I think one wants to be able to pop up and sort of recognize the appropriate level of confidence. But I think there's also a lot of value in just being like, I'm going to essentially assume, I'm gonna condition on this problem being possible or this being broadly the right approach. And I'm just gonna go and assume that for a while and go and work within that and push really hard on it. And if society has lots of people doing that for different things, that's actually really useful in terms of going and getting to, either really ruling things out, right? We can be like, well, that didn't work and we know that somebody tried hard or going and getting to something that does teach us something about the world. So another interesting hypothesis is the superposition hypothesis. Can you describe what superposition is? Yeah, so earlier we were talking about word defect, right? And we were talking about how, maybe you have one direction that corresponds to gender and maybe another that corresponds to royalty and another one that corresponds to Italy and another one that corresponds to food and all of these things. Well, oftentimes maybe these word embeddings, they might be 500 dimensions, a thousand dimensions. And so if you believe that all of those directions were orthogonal, then you could only have 500 concepts. And I love pizza, but if I was gonna go and give the 500 most important concepts in the English language, probably Italy wouldn't be, it's not obvious at least that Italy would be one of them, right? Because you have to have things like plural and singular and verb and noun and adjective. And there's a lot of things we have to get to before we get to Italy. And Japan, and there's a lot of countries in the world. And so how might it be that models could simultaneously have the linear representation hypothesis be true and also represent more things than they have directions? So what does that mean? Well, okay, so if linear representation hypothesis is true, something interesting has to be going on. Now I'll tell you one more interesting thing before we go and we do that, which is earlier we were talking about all these polysematic neurons, right? These neurons that, when we were looking at inception V1, there's these nice neurons that like the car detector and the curve detector and so on that respond to lots of, to very coherent things. But it's lots of neurons that respond to a bunch of unrelated things. And that's also an interesting phenomenon. And it turns out as well that even these neurons that are really, really clean, if you look at the weak activations, right? So if you look at like the activations where it's like activating 5 % of the maximum activation, it's really not the core thing that it's expecting, right? So if you look at a curve detector, for instance, and you look at the places where it's 5 % active, you could interpret it just as noise, or it could be that it's doing something else there. Okay, so how could that be? Well, there's this amazing thing in mathematics called compressed sensing. And it's actually this very surprising fact where if you have a high dimensional space and you project it into a low dimensional space, ordinarily, you can't go and sort of unprojected and get back your high dimensional vector, right? You threw information away. This is like, you know, you can't invert a rectangular matrix. You can only invert square matrices. But it turns out that that's actually not quite true. If I tell you that the high dimensional vector was sparse, so it's mostly zeros, then it turns out that you can often go and find back the high dimensional vector with very high probability. So that's a surprising fact, right? It says that, you know, you can have this high dimensional vector space and as long as things are sparse, you can project it down, you can have a lower dimensional projection of it and that works. So this research hypothesis is saying that that's what's going on in neural networks. For instance, that's what's going on in word embeddings. The word embeddings are able to simultaneously have directions be the meaningful thing. And by exploiting the fact that they're operating on a fairly high dimensional space, they're actually, and the fact that these concepts are sparse, right? Like, you know, you usually aren't talking about Japan and Italy at the same time. You know, most of those concepts, you know, in most sentences, Japan and Italy are both zero, they're not present at all. And if that's true, then you can go and have it be the case that you can have many more of these sort of directions that are meaningful, these features, than you have dimensions. And similarly, when we're talking about neurons, you can have many more concepts than you have neurons. So that's at a high level of superstition hypothesis. Now, it has this even wilder implication, which is to go and say that neural networks are, it may not just be the case that the representations are like this, but the computation may also be like this. You know, the connections between all of them. And so in some sense, neural networks may be shadows of much larger, sparser neural networks. And what we see are these projections. And the strongest version of superstition hypothesis would be to take that really seriously and sort of say, you know, there actually is in some sense this upstairs model, this, you know, where the neurons are really sparse and all interpretable and there's, you know, the weights between them are these really sparse circuits. And that's what we're studying. And the thing that we're observing is the shadow of it. And so we need to find the original object.

02:00:03 Speaker 2: And the process of learning is trying to construct a compression of the upstairs model that doesn't lose too much information in the projection.

02:00:12 Speaker 1: Yeah, it's finding how to fit it efficiently or something like this. The gradient descent is doing this. And in fact, so this sort of says that gradient descent, you know, it could just represent a dense neural network, but it sort of says that gradient descent is implicitly searching over the space of extremely sparse models that could be projected into this low dimensional space. And there's large body of work of people going and trying to study sparse neural networks, right? Where you go and you have, you could design neural networks, right? Where the edges are sparse and the activations are sparse. And, you know, my sense is that work has generally, it feels very principled, right? It makes so much sense. And yet that work hasn't really panned out that well is my impression broadly. And I think that a potential answer for that is that actually the neural network is already sparse in some sense. Gradient descent was the whole time gradient, you were trying to go and do this. Gradient descent was actually in the behind the scenes going and searching more efficiently than you could through the space of sparse models and going and learning whatever sparse model was most efficient. And then figuring out how to fold it down nicely to go and run conveniently on your GPU, which does, you know, it's a nice dense matrix multiplies. And that you just can't beat that. How many concepts do you think can be shoved into a neural network? Depends on how sparse they are. So there's probably an upper bound from the number of parameters, right? Cause you have to have, you still have to have, you know, weights that go and connect them together. So that's one upper bound. There are in fact, all these lovely results from compressed sensing and the Johnson -Lindenstrauss lemma and things like this, that they basically tell you that if you have a vector space and you want to have almost orthogonal vectors, which is sort of probably the thing that you want here, right? So you're going to say, well, you know, I'm going to give up on having my concepts, my features be strictly orthogonal, but I'd like them to not interfere that much. I'm going to have to ask them to be almost orthogonal. Then this would say that it's actually, you know, once you set a threshold for what you're willing to accept in terms of how much cosine similarity there is, that's actually exponential in the number of neurons that you have. So at some point, that's not going to even be the limiting factor, but there's some beautiful results there. And in fact, it's probably even better than that in some sense, because that's sort of for saying that, you know, any random set of features could be active. But in fact, the features have sort of a correlational structure where some features, you know, are more likely to co -occur and other ones are less likely to co -occur. And so neural networks, my guess would be, can do very well in terms of going and packing things in such to the point that's probably not the limiting factor. How does the problem of polysemanticity enter the picture here? Polysemanticity is this phenomenon we observe where you look at many neurons and the neuron doesn't just sort of represent one concept. It's not a clean feature. It responds to a bunch of unrelated things. And superstition is, you can think of as being a hypothesis that explains the observation of polysemanticity. So polysemanticity is this observed phenomenon and superstition is a hypothesis that would explain it along with some other things. So that makes McInturb more difficult. Right, so if you're trying to understand things in terms of individual neurons and you have polysemantic neurons, you're in an awful lot of trouble, right? I mean, the easiest answer is like, okay, well, you're looking at the neurons, you're trying to understand them. This one responds for a lot of things. It doesn't have a nice meaning. Okay, well, that's bad. Another thing you could ask is, ultimately we want to understand the weights. And if you have two polysematic neurons and each one responds to three things and then the other neuron responds to three things and you have a weight between them, what does that mean? Does it mean that like all three, like there's these nine interactions going on? It's a very weird thing. But there's also a deeper reason, which is related to the fact that neural networks operate on really high dimensional spaces. So I said that our goal was to understand neural networks and understand the mechanisms. And one thing you might say is like, well, why not? It's just a mathematical function. Why not just look at it, right? Like, one of the earliest projects I did studied these neural networks that mapped two -dimensional spaces to two -dimensional spaces. And you can sort of interpret them in this beautiful way as like bending manifolds. Why can't we do that? Well, as you have a higher dimensional space, the volume of that space in some senses is exponential in the number of inputs you have. And so you can't just go and visualize that. So we somehow need to break that apart. We need to somehow break that exponential space into a bunch of things that we, some non -exponential number of things that we can reason about independently. And the independence is crucial because it's the independence that allows you to not have to think about all the exponential combinations of things. And things being monosemantic, things only having one meaning, things having a meaning, that is the key thing that allows you to think about them independently. And so I think that's, if you want the deepest reason why we want to have interpretable monosemantic features, I think that's really the deep reason. And so the goal here,

02:05:00 Speaker 2: as your recent work has been aiming at, is how do we extract the monosemantic features from a neural net that has polysemantic features

02:05:08 Speaker 1: and all this mess? Yes, we have, we observe these polysematic neurons and we hypothesize that what's going on is superstition. And if superstition is what's going on, there's actually a sort of well -established technique that is sort of the principled thing to do, which is dictionary learning. And it turns out if you do dictionary learning, in particular, if you do sort of a nice efficient way that in some sense sort of nicely regularizes it as well, called a sparse autoencoder, if you train a sparse autoencoder, these beautiful interpretable features start to just fall out where there weren't any beforehand. And so that's not a thing that you would necessarily predict, right? But it turns out that that works very, very well. To me, that seems like some non -trivial validation of linear representations and superstition. So with dictionary learning, you're not looking for particular kinds of categories, you don't know what they are,

02:05:56 Speaker 2: they just emerge.

02:05:57 Speaker 1: And this gets back to our earlier point, right? When we're not making assumptions, gradient descent is smarter than us. So we're not making assumptions about what's there. I mean, one certainly could do that, right? One could assume that there's a PHP feature and go and search for it, but we're not doing that. We're saying we don't know what's going to be there. Instead, we're just going to go and let the sparse autoencoder discover the things that are there.

02:06:16 Speaker 2: So can you talk to the toward monosemanticity paper from October last year?

02:06:22 Speaker 1: That had a lot of like nice breakthrough results. That's very kind of you to describe it that way. Yeah, I mean, this was our first real success using sparse autoencoders. So we took a one -layer model, and it turns out if you go and you do dictionary learning on it, you find all these really nice interpretable features. So, you know, the Arabic feature, the Hebrew feature, the base 64 feature, those were some examples that we studied in a lot of depth and really showed that they were what we thought they were. It turns out if you train a model twice as well and train two different models and do dictionary learning, you find analogous features in both of them. So that's fun. You find all kinds of different features. So that was really just showing that this works. And, you know, I should mention that there was this Cunningham et al that had very similar results around the same time.

02:07:08 Speaker 2: There's something fun about doing these kinds of small -scale experiments and finding that it's actually working.

02:07:15 Speaker 1: Yeah, well, and there's so much structure here. Like, you know, so maybe stepping back for a while, I thought that maybe all this mechanistic interpretability work, the end result was going to be that I would have an explanation for why it was sort of, you know, very hard and not going to be tractable. You know, we'd be like, well, there's this problem with supersession, and it turns out supersession is really hard and we're kind of screwed. But that's not what happened. In fact, a very natural, simple technique just works. And so then that's actually a very good situation. You know, I think this is a sort of hard research problem and it's got a lot of research risk and, you know, it might still very well fail. But I think that some amount of, some very significant amount of research risk was sort of put behind us when that started to work. Can you describe what kind of features can be extracted in this way? Well, so it depends on the model that you're studying, right? So the larger the model, the more sophisticated they're going to be. And we'll probably talk about follow -up work in a minute. But in these one -layer models, so some very common things I think were languages, both programming languages and natural languages. There were a lot of features that were specific words in specific contexts. So the, and I think really the way to think about this is that the is likely about to be followed by a noun. So it's really, you could think of this as the feature, but you could also think of this as predicting a specific noun feature. And there would be these features that would fire for the in the context of say a legal document or a mathematical document or something like this. And so, maybe in the context of math, you're like the, and then predict vector or matrix, all of these mathematical words. Whereas in other contexts, you would predict other things. That was common.

02:08:54 Speaker 2: And basically we need clever humans to assign labels to what we're seeing. Yes.

02:09:00 Speaker 1: So this is, the only thing this is doing is it's sort of unfolding things for you. So if everything was sort of folded over top of it, C Resolution folded everything on top of itself and you can't really see it, this is unfolding it. But now you still have a very complex thing to try to understand. So then you have to do a bunch of work understanding what these are. And some of them are really subtle. Like there's some really cool things, even in this one -layer model about Unicode, where of course some languages are in Unicode and the tokenizer won't necessarily have a dedicated token for every Unicode character. So instead what you'll have is you'll have these patterns of alternating token or alternating tokens that each represent half of a Unicode character. And then you have a different feature that goes and activates on the opposing ones to be like, okay, I just finished a character, go and predict next prefix. Then, okay, I'm on the prefix, predict a reasonable suffix. And you have to alternate back and forth. So there's, these one -layer models are really interesting. And I mean, there's another thing that just, you might think, okay, there would just be one Base64 feature, but it turns out there's actually a bunch of Base64 features because you can have English text encoded as Base64. And that has a very different distribution of Base64 tokens than regular. And there's some things about tokenization as well that it can exploit. And I don't know, there's all kinds of fun stuff.

02:10:21 Speaker 2: How difficult is the task of sort of assigning labels to what's going on? Can this be automated by AI?

02:10:28 Speaker 1: Well, I think it depends on the feature and it also depends on how much you trust your AI. So there's a lot of work doing automated interoperability. I think that's a really exciting direction. And we do a fair amount of automated interoperability and have Claude go and label our features. Is there some funny moments where it's totally right or it's totally wrong? Yeah, well, I think it's very common that it's like says something very general, which is like true in some sense, but not really picking up on the specific of what's going on. So I think that's a pretty common situation. You don't know that I have a particularly amusing one. That's interesting, that little gap between it is true,

02:11:09 Speaker 2: but it doesn't quite get to the deep nuance of a thing. Yeah. That's a general challenge. It's like, it's certainly an incredible accomplishment that can say a true thing, but it doesn't, it's missing the depth sometimes. And in this context, it's like the arc challenge, you know, the sort of IQ type of tests. It feels like figuring out what a feature represents

02:11:33 Speaker 1: is a little puzzle you have to solve. Yeah, and I think that sometimes they're easier and sometimes they're harder as well. So yeah, I think that's tricky. There's another thing which I don't know, maybe in some ways this is my like aesthetic coming in, but I'll try to give you a rationalization. I'm actually a little suspicious of automated interoperability. And I think that's partly just that I want humans to understand neural networks. And if the neural network is understanding it for me, you know, I'm not, I don't quite like that. But I do have a bit of, in some ways I'm sort of like the mathematicians who are like, if there's a computer automated proof, it doesn't count. They won't understand it. But I do also think that there's kind of like reflections on trusting trust type issue, where if you, there's this famous talk about, you know, like when you're writing a computer program, you have to trust your compiler. And if there was like malware in your compiler, then it could go and inject malware into the next compiler. And, you know, you'd be kind of in trouble, right? Well, if you're using neural networks to go and verify that your neural networks are safe, the hypothesis that you're testing for is like, okay, well, the neural network maybe isn't safe. And you have to worry about like, is there some way that it could be screwing with you? So, you know, I think that's not a big concern now, but I do wonder in the long run, if we have to use really powerful AI systems to go and, you know, audit our AI systems, is that actually something we can trust? But maybe I'm just rationalizing, because I just want us to have to get to a point where humans understand everything.

02:12:58 Speaker 2: Yeah, I mean, especially, that's hilarious, especially as we talk about AI safety and looking for features that would be relevant to AI safety, like deception and so on. So let's talk about the scaling monosemanticity paper in May, 2024. Okay, so what did it take to scale this, to apply to Cloud 3, Sonic?

02:13:18 Speaker 1: Well, a lot of GPUs. A lot more GPUs. But one of my teammates, Tom Hennigan, was involved in the original scaling loss work. And something that he was sort of interested in from very early on is, are there scaling laws for interoperability? And so something he sort of immediately did when this work started to succeed, and we started to have sparse autoencoders work, was he became very interested in, what are the scaling laws for making sparse autoencoders larger, and how does that relate to making the base model larger? And so it turns out this works really well, and you can use it to sort of project, if you train a sparse autoencoder at a given size, how many tokens should you train on, and so on. So this was actually a very big help to us in scaling up this work, and made it a lot easier for us to go and train really large sparse autoencoders, where it's not like training the big models, but it's starting to get to a point where it's actually expensive to go and train the really big ones.

02:14:21 Speaker 2: So you have to do all this stuff

02:14:23 Speaker 1: of like splitting it across large GPUs.

02:14:26 : Oh yeah, no, I mean,

02:14:26 Speaker 1: there's a huge engineering challenge here too, right? So yeah, so there's a scientific question of how do you scale things effectively, and then there's an enormous amount of engineering to go and scale this up. So you have to chart it, you have to think very carefully about a lot of things. I'm lucky to work with a bunch of great engineers, because I am definitely not a great engineer.

02:14:43 Speaker 2: Yeah, and the infrastructure especially, yeah, for sure.

02:14:45 Speaker 1: So it turns out, TODR, it worked. It worked, yeah. And I think this is important, because you could have imagined a world where you set after towards monosemanticity. Chris, this is great, it works on a one -layer model, but one -layer models are really idiosyncratic. Like maybe that's just something, maybe the linear representation hypothesis and superposition hypothesis is the right way to understand a one -layer model, but it's not the right way to understand larger models. And so I think, I mean, first of all, the Cunningham et al paper sort of cut through that a little bit and sort of suggested that this wasn't the case, but scaling monosemanticity sort of, I think was significant evidence that even for very large models, and we did it on Claude's Resona, which at that point was one of our production models, even these models seem to be very, seem to be substantially explained at least by linear features and doing dictionary learning on the works, and as you learn more features, you go and you explain more and more. So that's, I think, quite a promising sign. And you find now really fascinating abstract features. And the features are also multimodal, they respond to images and text for the same concept, which is fun.

02:15:55 Speaker 2: Yeah, can you explain that? I mean, like, you know, backdoor, there's just a lot of examples that you can -

02:16:01 Speaker 1: Yeah, so maybe let's start with one example to start, which is we found some features around sort of security vulnerabilities and backdoors in code. So it turns out those are actually two different features. So there's a security vulnerability feature, and if you force it active, Claude will start to go and write security vulnerabilities like buffer overflows into code. And it also, it fires for all kinds of things like, you know, some of the top dataset examples for it were things like, you know, dash dash disable, you know, SSL or something like this, which are sort of obviously really, really insecure.

02:16:34 Speaker 2: So at this point, it's kind of like, maybe it's just because the examples are presented that way, it's kind of like surface, a little bit more obvious examples, right? I guess the idea is that down the line, it might be able to detect more nuanced, like deception or bugs or that kind of stuff.

02:16:50 Speaker 1: Yeah, well, maybe I want to distinguish two things. So one is the complexity of the feature or the concept, right? And the other is the nuance of the, how subtle the examples we're looking at, right? So when we show the top dataset examples, those are the most extreme examples that cause that feature to activate. And so it doesn't mean that it doesn't fire for more subtle things. So the insecure code feature, you know, the stuff that it fires for most strongly for are these like really obvious, you know, disable the security type things. But, you know, it also fires for, you know, buffer overflows and more subtle security vulnerabilities in code. You know, these features are all multimodal. So you could ask like, what images activate this feature? And it turns out that the security vulnerability feature activates for images of like people clicking on Chrome to like go past the, like, you know, this website, the SSL certificate might be wrong or something like this. Another thing that's very entertaining is there's backdoors in code feature. Like you activate it, it goes on, Cloud writes a backdoor that like will go and dump your data to a port or something. But you can ask, okay, what images activate the backdoor feature? It was devices with hidden cameras in them. So there's a whole, apparently, genre of people going and selling devices that look innocuous, that have hidden cameras, and they have ads about how there's a hidden camera in it. And I guess that is the, you know, physical version of a backdoor. And so it sort of shows you how abstract these concepts are, right? And I just thought that was, I'm sort of sad that there's a whole market of people selling devices like that, but I was kind of delighted that that was the thing that it came up with as the top image examples for the feature. Yeah, it's nice.

02:18:36 Speaker 2: It's multimodal, it's multi almost context, it's broad, strong definition of a singular concept. It's nice. Yeah. To me, one of the really interesting features, especially for AI safety is deception and lying. And the possibility that these kinds of methods could detect lying in a model, especially gets smarter and smarter and smarter. Presumably, that's a big threat of a super intelligent model that it can deceive the people operating it as to its intentions or any of that kind of stuff. So what have you learned from detecting lying inside models?

02:19:13 Speaker 1: Yeah, so I think we're in some ways in early days for that. We find quite a few features related to deception and lying. There's one feature where it fires for people lying and being deceptive and you force it active and Claude starts lying to you. So we have a deception feature. I mean, there's all kinds of other features about withholding information and not answering questions, features about power seeking and coups and stuff like that. So there's a lot of features that are kind of related to spooky things. And if you force them active, Claude will behave in ways that are not the kinds of behaviors you want.

02:19:50 Speaker 2: What are possible next exciting directions to you in the space of Macintype?

02:19:56 Speaker 4: Well, there's a lot of things. So for one thing,

02:20:03 Speaker 1: I would really like to get to a point where we have circuits, where we can really understand not just the features, but then use that to understand the computation of models. That really for me is the ultimate goal of this. And there's been some work, we put out a few things. There's a paper from Sam Marks that does some stuff like this. So there's been some, I'd say some work around the edges here, but I think there's a lot more to do. And I think that will be a very exciting thing. That's related to a challenge we call interference weights, where due to superposition, if you just sort of naively look at whether features are connected together, there may be some weights that sort of don't exist in the upstairs model, but are just sort of artifacts of superposition. So that's a sort of technical challenge related to that. I think another exciting direction is just, you might think of sparse autoencoders as being kind of like a telescope. They allow us to look out and see all these features that are out there. And as we build better and better sparse autoencoders, get better and better at dictionary learning, we see more and more stars. And we zoom in on smaller and smaller stars. But there's kind of a lot of evidence that we're only still seeing a very small fraction of the stars. There's a lot of matter in our neural network universe that we can't observe yet. And it may be that we'll never be able to have fine enough instruments to observe it. And maybe some of it just isn't possible, isn't computationally tractable to observe. And so there's sort of a kind of dark matter and not in maybe the sense of modern astronomy, but of earlier astronomy when we didn't know what this unexplained matter is. And so I think a lot about that dark matter and whether we'll ever observe it and what that means for safety if we can't observe it, if some significant fraction of neural networks are not accessible to us. Another question that I think a lot about is at the end of the day, mechanistic interpolate is this very microscopic approach to interpolate. It's trying to understand things in a very fine grained way. But a lot of the questions we care about are very macroscopic. We care about these questions about neural network behavior. I think that's the thing that I care most about, but there's lots of other sort of larger scale questions you might care about. And somehow, the nice thing about having a very microscopic approach is it's maybe easier to ask, is this true? But the downside is it's much further from the things we care about. And so we now have this ladder to climb. And I think there's a question of, will we be able to find, are there sort of larger scale abstractions that we can use to understand neural networks that we get up from this very microscopic approach?

02:22:48 Speaker 2: Yeah, you've written about this, this kind of organs question. Yeah, exactly. If we think of interpretability as a kind of anatomy of neural networks, most of the circus threads involve studying tiny little veins, looking at the small scale and individual neurons and how they connect. However, there are many natural questions that the small scale approach doesn't address. In contrast, the most prominent abstractions in biological anatomy involve larger scale structures like individual organs, like the heart or entire organ systems, like the respiratory system. And so we wonder, is there a respiratory system or heart or brain region of an artificial neural network? Yeah, exactly.

02:23:30 Speaker 1: And I mean, like if you think about science, right, a lot of scientific fields have, you know, investigate things at many level abstractions. In biology, you have like, you know, molecular biology studying, you know, proteins and molecules and so on. And they have cellular biology and then you have histology studying tissues. Then you have anatomy and then you have zoology and then you have ecology. And so you have many, many levels of abstraction or, you know, physics, maybe you have the physics of individual particles and then, you know, statistical physics gives you thermodynamics and things like this. And so you often have different levels of abstraction. And I think that right now we have, you know, mechanistic interpretability if it succeeds is sort of like a microbiology of neural networks, but we want something more like anatomy. And so, and, you know, a question you might ask is why, why can't you just go there directly? And I think the answer is superposition, at least in significant part, it's that it's actually very hard to see this macroscopic structure without first sort of breaking down the microscopic structure in the right way and then studying how it connects together. But I'm hopeful that there is gonna be something much larger than features and circuits and that we're gonna be able to have a story that's much, that involves much bigger things. And then you can sort of study in detail the parts you care about. As opposed to neurobiology, like a psychologist or a psychiatrist of a neural network. And I think that the beautiful thing would be if we could go and rather than having disparate fields for those two things, if you could have a, build a bridge between them such that you could go and have all of your higher level abstractions be grounded very firmly in this very solid, you know, more rigorous ideally foundation.

02:25:11 Speaker 2: What do you think is the difference between the human brain, the biological neural network and the artificial neural network?

02:25:17 Speaker 1: Well, the neuroscientists have a much harder job than us. You know, sometimes I just like count my blessings by how much easier my job is than the neuroscientists, right? So I have, we can record from all the neurons. We can do that on arbitrary amounts of data. The neurons don't change while you're doing that, by the way, you can go and ablate neurons, you can edit the connections and so on. And then you can undo those changes. That's pretty great. You can force any, you can intervene on any neuron and force it active and see what happens. You know which neurons are connected to everything, right? Neuroscientists wanna get the connectome, we have the connectome and we have it for like much bigger than C. elegans. And then not only do we have the connectome, we know what the, you know, which neurons excite or inhibit each other, right? So we have, it's not just that we know that like the binary mask, we know the weights. We can take gradients. We know computationally what each neuron does. So I don't know, the list goes on and on. We just have so many advantages over neuroscientists. And then just by having all those advantages, it's really hard. And so one thing I do sometimes think is like, gosh, like if it's this hard for us, it seems impossible under the constraints of neuroscience or, you know, near impossible. I don't know, maybe part of me is like, I've got a few neuroscientists on my team. Maybe I'm sort of like, ah, you know, the maybe the neuroscientists, maybe some of them would like to have an easier problem. That's still very hard. And they could come and work on neural networks. And then after we figure out things in sort of the easy little pond of trying to understand neural networks, which is still very hard, then we could go back to biological neuroscience.

02:26:51 Speaker 2: I love what you've written about the goal of McInterp research as two goals, safety and beauty. So can you talk about the beauty side of things?

02:27:00 Speaker 1: Yeah, so, you know, there's this funny thing where I think some people want, some people are kind of disappointed by neural networks, I think, where they're like, ah, you know, neural networks, it's just these simple rules. And then you just like do a bunch of engineering to scale it up and it works really well. And like, where's the like complex ideas? You know, this isn't like a very nice, beautiful scientific result. And I sometimes think when people say that, I picture them being like, you know, evolution is so boring. It's just a bunch of simple rules and you run evolution for a long time and you get biology. Like what a sucky, you know, way for biology to have turned out, where's the complex rules? But the beauty is that the simplicity generates complexity. You know, biology has these simple rules and it gives rise to, you know, all the life and ecosystems that we see around us, all the beauty of nature that all just comes from evolution and from something very simple evolution. And similarly, I think that neural networks build, create enormous complexity and beauty inside it. And I think that there's a lot of beauty and structure inside themselves that people generally don't look at and don't try to understand. Because it's hard to understand. But I think that there is an incredibly rich structure to be discovered inside neural networks. A lot of very deep beauty. And if we're just willing to take the time to go and see it and understand it.

02:28:20 Speaker 2: Yeah, I love McInterp. The feeling like we are understanding or getting glimpses of understanding the magic that's going on inside is really wonderful.

02:28:30 Speaker 1: It feels to me like one of the questions is just calling out to be asked. And I'm sort of, I mean, a lot of people are thinking about this, but I'm often surprised that not more are. Is how is it that we don't know how to create computer systems that can do these things? And yet we have these amazing systems that we don't know how to directly create computer programs that can do these things. But these neural networks can do all these amazing things. And it just feels like that is obviously the question that sort of is calling out to be answered. If you are, if you have any degree of curiosity, it's like, how is it that humanity now has these artifacts that can do these things that we don't know how to do?

02:29:07 Speaker 2: Yeah, I love the image of the circus reaching towards the light of the objective function. Yeah, it's just, it's this organic thing that we've grown and we have no idea what we've grown. Well, thank you for working on safety and thank you for appreciating the beauty of the things you discover. And thank you for talking today, Chris.

02:29:23 : This was wonderful.

02:29:23 Speaker 1: Thank you for taking the time to chat as well.

02:29:26 Speaker 2: Thanks for listening to this conversation with Chris Ola. And before that with Dari Amadei and Amanda Askel. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Alan Watts. The only way to make sense out of change is to plunge into it, move with it and join the dance.

02:29:48 Speaker 4: Thank you for listening and hope to see you next time.